{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T04:35:49.327495Z",
     "start_time": "2024-11-02T04:35:45.584982Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 64)\n",
    "        self.layer2 = nn.Linear(64, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        return self.layer2(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T04:35:49.331862Z",
     "start_time": "2024-11-02T04:35:49.328506Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T04:35:49.345816Z",
     "start_time": "2024-11-02T04:35:49.332876Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, C, env):\n",
    "    # initialize environment\n",
    "    self.env = env\n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = []\n",
    "    self.capacity = N\n",
    "    self.pointer = 0\n",
    "    state, info = env.reset()\n",
    "    if isinstance(state, int):\n",
    "        state = [state]\n",
    "        print(\"here: \", len(state))\n",
    "    \n",
    "    self.policy_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    self.target_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "\n",
    "    self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.1)\n",
    "    self.C = C\n",
    "\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    for i in range(episodes):\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      state, info = self.env.reset()\n",
    "      if isinstance(state, int):\n",
    "        state = torch.tensor([state],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "      else:\n",
    "        state = torch.tensor(state,device=device, dtype=torch.float32).unsqueeze(0)\n",
    "      stopped = False\n",
    "      rewards = steps = 0\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      while not stopped:\n",
    "        # Select action and observe reward\n",
    "        action_type = action_function(state, eps)\n",
    "        observation, reward, terminated, truncated, _ = self.env.step(action_type)\n",
    "        # For Lake\n",
    "        reward = reward if reward != 0 else -1\n",
    "        self.env.render()\n",
    "        # Form the next action\n",
    "        if isinstance(observation, int):\n",
    "            next_state = torch.tensor([observation],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Encode action type for ease of use\n",
    "        action_type = torch.tensor([action_type], device=device, dtype=torch.int64)\n",
    "        # store transition in replay buffer\n",
    "        transition = state, action_type,  next_state, reward\n",
    "        state = next_state\n",
    "        \n",
    "        if self.pointer < self.capacity:\n",
    "            self.replay.append(transition)\n",
    "        else:\n",
    "            self.replay[self.pointer % self.capacity] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "        # When terminated store the last value found\n",
    "        stopped = terminated or truncated\n",
    "        reward = -10 if reward == -1 and stopped else reward\n",
    "        if stopped:\n",
    "            transition = state, action_type,  None, reward\n",
    "            if self.pointer < self.capacity:\n",
    "                self.replay.append(transition)\n",
    "            else:\n",
    "                self.replay[self.pointer % self.capacity] = transition\n",
    "            self.pointer += 1\n",
    "            \n",
    "        # Add rewards to the count\n",
    "        rewards += reward\n",
    "        # Run the replay function\n",
    "        self.replay_function(discount ** steps)\n",
    "        # Every C steps update the target function\n",
    "        if steps % self.C == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        steps += 1\n",
    "    \n",
    "      print(\"Episode: \", i, \" Reward: \", rewards)\n",
    "      total_reward[i] = rewards  \n",
    "    self.env.close()\n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "      \n",
    "  def classic_action(self, state, epsilon):\n",
    "      if isinstance(state, int):\n",
    "        state = torch.tensor([state],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "      if np.random.rand() < epsilon:\n",
    "        action_type = self.env.action_space.sample()\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "            action_type = self.policy_net(state).max(1).indices.item()\n",
    "      return action_type\n",
    "  \n",
    "  def replay_function(self, discount):\n",
    "    BATCH_SIZE = 128\n",
    "    if len(self.replay) < BATCH_SIZE:\n",
    "        return\n",
    "    else:\n",
    "        sample = random.sample(self.replay, k=BATCH_SIZE)\n",
    "        Q_list = torch.tensor([] , device=device)\n",
    "        target_list = torch.tensor([] , device=device)\n",
    "        action_list = torch.tensor([], dtype=torch.int64, device=device)\n",
    "        for state, action, next_state, reward in sample:\n",
    "            if next_state is None:\n",
    "                Q_list = torch.cat((Q_list, self.policy_net(state)))\n",
    "                # Make an actions array\n",
    "                action_list = torch.cat((action_list, action), 0)\n",
    "\n",
    "                # Calculate updated Q value\n",
    "                Q_val = torch.tensor([reward * dic], device=device)\n",
    "                # Add value to expected target list\n",
    "                target_list = torch.cat((target_list, Q_val))\n",
    "            else:\n",
    "                    # Take entire Q row\n",
    "                    Q_list = torch.cat((Q_list, self.policy_net(state)))\n",
    "                    # Make an actions array\n",
    "                    action_list = torch.cat((action_list, action), 0)\n",
    "                    # Take max expected Q from the target network\n",
    "                    max_expected = self.target_net(next_state).max(1).values\n",
    "\n",
    "                    # Calculate updated Q value\n",
    "                    Q_val = torch.tensor([(max_expected * discount) + reward], device=device)\n",
    "                    # Add value to expected target list\n",
    "                    target_list = torch.cat((target_list, Q_val))\n",
    "\n",
    "\n",
    "\n",
    "        # Apply the action list to get real expected Q values\n",
    "        selected_q_values = Q_list.gather(1, action_list.unsqueeze(1))\n",
    "        loss_function = nn.SmoothL1Loss()\n",
    "\n",
    "        \n",
    "        loss = loss_function(selected_q_values, target_list.unsqueeze(1))\n",
    "        #backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(\"pickles/\" + filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T04:35:49.351991Z",
     "start_time": "2024-11-02T04:35:49.346829Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints the reward per epsisode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - abs(int(min(reward_per_episode)) * (.2))\n",
    "    maxs = int(max(reward_per_episode)) + abs(int(max(reward_per_episode)) * (.3) )\n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here:  1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m discount \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.95\u001B[39m\n\u001B[0;32m     10\u001B[0m action \u001B[38;5;241m=\u001B[39m Lake\u001B[38;5;241m.\u001B[39mclassic_action\n\u001B[1;32m---> 11\u001B[0m total_rewards \u001B[38;5;241m=\u001B[39m \u001B[43mLake\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mmax\u001B[39m(total_rewards))\n\u001B[0;32m     13\u001B[0m Lake\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrpreisl_angustsa_assignment2_part2_dqn_FrozenLake.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[3], line 41\u001B[0m, in \u001B[0;36mDQN.train\u001B[1;34m(self, episodes, epsilon, discount, action_function, greedy)\u001B[0m\n\u001B[0;32m     38\u001B[0m action_type \u001B[38;5;241m=\u001B[39m action_function(state, eps)\n\u001B[0;32m     39\u001B[0m observation, reward, terminated, truncated, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action_type)\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Form the next action\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(observation, \u001B[38;5;28mint\u001B[39m):\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:409\u001B[0m, in \u001B[0;36mOrderEnforcing.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disable_render_order_enforcing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    408\u001B[0m     )\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\core.py:332\u001B[0m, in \u001B[0;36mWrapper.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RenderFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[RenderFrame] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:303\u001B[0m, in \u001B[0;36mPassiveEnvChecker.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_render_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv)\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:228\u001B[0m, in \u001B[0;36mCliffWalkingEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    226\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_render_text()\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_render_gui\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender_mode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:315\u001B[0m, in \u001B[0;36mCliffWalkingEnv._render_gui\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    313\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[0;32m    314\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m--> 315\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# rgb_array\u001B[39;00m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mtranspose(\n\u001B[0;32m    318\u001B[0m         np\u001B[38;5;241m.\u001B[39marray(pygame\u001B[38;5;241m.\u001B[39msurfarray\u001B[38;5;241m.\u001B[39mpixels3d(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwindow_surface)), axes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    319\u001B[0m     )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "C = 10\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "env.reset()\n",
    "Lake = DQN(N, C, env)\n",
    "\n",
    "episodes = 100\n",
    "epsilon = .95\n",
    "discount = .9\n",
    "action = Lake.classic_action\n",
    "total_rewards = Lake.train(episodes, epsilon, discount, action, False)\n",
    "print(\"Best reward: \", max(total_rewards))\n",
    "Lake.save(\"drpreisl_angustsa_assignment2_part2_dqn_FrozenLake.pickle\")\n",
    "reward_print(total_rewards, episodes, \"FrozenLake\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = Lake.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"FrozenLake\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-02T04:35:49.353Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "N = 5000\n",
    "C = 3\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.reset()\n",
    "cart = DQN(N, C, env)\n",
    "\n",
    "episodes = 300\n",
    "epsilon = .99\n",
    "discount = 1.05\n",
    "action = cart.classic_action\n",
    "total_rewards = cart.train(episodes, epsilon, discount, action, False)\n",
    "cart.save(\"drpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\")\n",
    "reward_print(total_rewards, episodes, \"CartPole\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = cart.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"CartPole\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T04:36:06.265612Z",
     "start_time": "2024-11-02T04:36:06.265612Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(cart.target_net[()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-02T04:36:06.266609Z"
    }
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
