{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T07:06:31.471926Z",
     "start_time": "2024-10-31T07:06:31.467884Z"
    }
   },
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "\n",
    "      self.observation_space = spaces.Discrete(36)\n",
    "      self.action_space = spaces.Discrete(6)\n",
    "      self.max_timesteps = 150\n",
    "\n",
    "      self.timestep = 0\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.goal_pos = [5, 5]\n",
    "      self.package_pos = [3, 1]\n",
    "      self.state = np.zeros((6,6))\n",
    "      #Shelves\n",
    "      self.wall_1 = [3,2]\n",
    "      self.wall_2 = [4,2]\n",
    "      self.wall_3 = [5,2]\n",
    "      self.wall_4 = [1,4]\n",
    "      self.wall_5 = [1,5]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "\n",
    "      self.package = 0\n",
    "      self.pickup = 0\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.timestep = 0\n",
    "      observation = self.state.flatten()\n",
    "      self.package =0\n",
    "      info = {}\n",
    "\n",
    "      return observation, info\n",
    "\n",
    "    # Step function: Contains the implementation for what happens when an\n",
    "    # agent takes a step in the environment.\n",
    "    def step(self, action):\n",
    "      prev = self.agent_pos.copy()\n",
    "      if action == 0: #down\n",
    "        self.agent_pos[0] += 1\n",
    "      if action == 1: #up\n",
    "        self.agent_pos[0] -= 1\n",
    "      if action == 2: #right\n",
    "        self.agent_pos[1] += 1\n",
    "      if action == 3: #left\n",
    "        self.agent_pos[1] -= 1\n",
    "\n",
    "      reward = -1\n",
    "      if action == 4: # Pick up\n",
    "        if np.array_equal(self.agent_pos, self.package_pos) and self.package == 0: #Picked up, in right location\n",
    "          self.package = 1\n",
    "          reward = 40\n",
    "          #print(\"picked up\", self.timestep)\n",
    "          #self.pickup += 1\n",
    "        elif self.package: #Picked up while holding a package\n",
    "          reward = -100\n",
    "        else: # Picked up in wrong location\n",
    "          reward = -10\n",
    "\n",
    "      if action == 5:\n",
    "        #Drop off\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and self.package == 1: #Dropped off in right location\n",
    "          reward = 100\n",
    "          #print(\"dropped off\", self.timestep)\n",
    "          self.package = 0\n",
    "          self.timestep = self.max_timesteps\n",
    "        elif self.package == 0: #Dropped off without holding a package\n",
    "          reward = -100\n",
    "        else: #dropped off in wrong location\n",
    "          reward -10\n",
    "\n",
    "      if np.array_equal(self.agent_pos, self.wall_1) or np.array_equal(self.agent_pos, self.wall_2) or np.array_equal(self.agent_pos, self.wall_3) or np.array_equal(self.agent_pos, self.wall_4) or np.array_equal(self.agent_pos, self.wall_5):\n",
    "        reward = -20\n",
    "        self.agent_pos = prev\n",
    "        #print(\"wall\")\n",
    "\n",
    "      # Comment this to demonstrate the truncation condition.\n",
    "      if self.agent_pos[0] > 5 or self.agent_pos[0] < 0 or self.agent_pos[1] > 5 or self.agent_pos[1] < 0:\n",
    "        reward = -25\n",
    "        #print(\"bounding: \", self.agent_pos)\n",
    "      self.agent_pos = np.clip(self.agent_pos, 0, 5)\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      observation = self.state.flatten()\n",
    "\n",
    "\n",
    "      self.timestep += 1\n",
    "\n",
    "      # Condition to check for termination (episode is over)\n",
    "      terminated = True if self.timestep >= self.max_timesteps else False\n",
    "\n",
    "      # Condition to check if agent is traversing to a cell beyond the permitted cells\n",
    "      # This helps the agent to learn how to behave in a safe and predictable manner\n",
    "      truncated = True if np.all((np.asarray(self.agent_pos) >=0 ) & (np.asarray(self.agent_pos) <= 6)) else False\n",
    "      #print(self.agent_pos)\n",
    "\n",
    "      return observation, reward, terminated, self.package\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "      plt.title('Grid Environment')\n",
    "      plt.imshow(self.state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T06:23:29.800820Z",
     "start_time": "2024-10-31T06:23:29.787676Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 144)\n",
    "        self.layer2 = nn.Linear(144, 144)\n",
    "        self.layer3 = nn.Linear(144, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T06:46:07.234814Z",
     "start_time": "2024-10-31T06:46:07.230768Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T06:46:09.483149Z",
     "start_time": "2024-10-31T06:46:09.479230Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:26:09.818498Z",
     "start_time": "2024-10-31T21:26:09.782969Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, C):\n",
    "    # initialize environment\n",
    "    self.env = GridEnvironment()\n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = ReplayMemory(N)\n",
    "    self.pointer = 0\n",
    "    self.policy_net = Net(2, 6).to(device)\n",
    "    self.target_net = Net(2, 6).to(device)\n",
    "\n",
    "    self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "\n",
    "    self.C = C\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    TAU = .0004\n",
    "    for i in range(episodes):\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      seq  = [None , None]\n",
    "      seq[0] = torch.tensor([0, 0],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "      terminated = False\n",
    "      pos = package = rewards = steps = 0\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      self.env.reset()\n",
    "      while not terminated:\n",
    "        # Select action\n",
    "        action_type = action_function(eps, pos, package)\n",
    "        state = torch.tensor([pos, package], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        # Set sequence\n",
    "        seq[1] = state\n",
    "        # Execute action and observe reward\n",
    "        position, reward, terminated, package = self.env.step(action_type.item())\n",
    "        pos = np.where(position == 50)[0][0]\n",
    "        rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        # store transition in replay buffer\n",
    "        self.replay.push(seq[0], action_type,  seq[1], reward)\n",
    "        seq[0] = state\n",
    "        \n",
    "        self.optimize_function(discount)\n",
    "        \n",
    "        if steps % self.C == 0:\n",
    "            target_net_state_dict = self.target_net.state_dict()\n",
    "            policy_net_state_dict = self.policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]* TAU + target_net_state_dict[key]*(1- TAU)\n",
    "            \n",
    "    \n",
    "            self.target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        steps += 1\n",
    "    \n",
    "      # Decay epsilon after every episode\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "  def warehouse_action(self, epsilon, pos, package):\n",
    "      if np.random.rand() < epsilon:\n",
    "        action_type = np.random.randint(6)\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor([pos, package], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "            return self.policy_net(state).max(1).indices.view(1, 1) \n",
    "      return torch.tensor([[action_type]], device=device, dtype=torch.long)\n",
    "      \n",
    "  def optimize_function(self, discount):\n",
    "    BATCH_SIZE = 128\n",
    "    if len(self.replay) < BATCH_SIZE:\n",
    "        return\n",
    "    else:\n",
    "        transitions = self.replay.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        expected_state_action_values = (next_state_values * discount) + reward_batch\n",
    "             \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "              \n",
    "\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(\"pickles/\" + filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T19:19:19.861195Z",
     "start_time": "2024-10-31T19:19:19.854999Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints the reward per episode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - int(min(reward_per_episode)) * (.15)\n",
    "    maxs = int(max(reward_per_episode)) + int(max(reward_per_episode)) * (.3) \n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "N = 5000\n",
    "C = 10\n",
    "env = GridEnvironment()\n",
    "agent = DQN(N, C)\n",
    "\n",
    "episodes = 1500\n",
    "epsilon = .9975\n",
    "discount = .4\n",
    "action = agent.warehouse_action\n",
    "total_rewards = agent.train(episodes, epsilon, discount, action, False)\n",
    "agent.save(\"drpreisl_angustsa_assignment2_part2_dqn_gridworld.pickle\")\n",
    "reward_print(total_rewards, episodes, \"grid world\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = agent.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"grid world\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-31T21:26:17.883992Z"
    }
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
