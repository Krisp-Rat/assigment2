{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T21:20:56.753612Z",
     "start_time": "2024-11-02T21:20:56.653123Z"
    }
   },
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "\n",
    "      self.observation_space = spaces.Discrete(36)\n",
    "      self.action_space = spaces.Discrete(6)\n",
    "      self.max_timesteps = 150\n",
    "\n",
    "      self.timestep = 0\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.goal_pos = [5, 5]\n",
    "      self.package_pos = [3, 1]\n",
    "      self.state = np.zeros((6,6))\n",
    "      #Shelves\n",
    "      self.wall_1 = [3,2]\n",
    "      self.wall_2 = [4,2]\n",
    "      self.wall_3 = [5,2]\n",
    "      self.wall_4 = [1,4]\n",
    "      self.wall_5 = [1,5]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "\n",
    "      self.package = 0\n",
    "      self.pickup = 0\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.timestep = 0\n",
    "      observation = self.state.flatten()\n",
    "      self.package =0\n",
    "      info = {}\n",
    "\n",
    "      return observation, info\n",
    "\n",
    "    # Step function: Contains the implementation for what happens when an\n",
    "    # agent takes a step in the environment.\n",
    "    def step(self, action):\n",
    "      prev = self.agent_pos.copy()\n",
    "      if action == 0: #down\n",
    "        self.agent_pos[0] += 1\n",
    "      if action == 1: #up\n",
    "        self.agent_pos[0] -= 1\n",
    "      if action == 2: #right\n",
    "        self.agent_pos[1] += 1\n",
    "      if action == 3: #left\n",
    "        self.agent_pos[1] -= 1\n",
    "\n",
    "      reward = -1\n",
    "      if action == 4: # Pick up\n",
    "        if np.array_equal(self.agent_pos, self.package_pos) and self.package == 0: #Picked up, in right location\n",
    "          self.package = 1\n",
    "          reward = 200\n",
    "          print(\"\\n----picked up----\\n\", self.timestep)\n",
    "          #self.pickup += 1\n",
    "        elif self.package: #Picked up while holding a package\n",
    "          reward = -150\n",
    "        else: # Picked up in wrong location\n",
    "          reward = -100\n",
    "\n",
    "      if action == 5:\n",
    "        #Drop off\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and self.package == 1: #Dropped off in right location\n",
    "          reward = 200\n",
    "          print(\"----dropped off----\", self.timestep)\n",
    "          self.package = 0\n",
    "          self.timestep = self.max_timesteps\n",
    "        elif self.package == 0: #Dropped off without holding a package\n",
    "          reward = -150\n",
    "        else: #dropped off in wrong location\n",
    "          reward = -100\n",
    "\n",
    "      if np.array_equal(self.agent_pos, self.wall_1) or np.array_equal(self.agent_pos, self.wall_2) or np.array_equal(self.agent_pos, self.wall_3) or np.array_equal(self.agent_pos, self.wall_4) or np.array_equal(self.agent_pos, self.wall_5):\n",
    "        reward = -200\n",
    "        self.agent_pos = prev\n",
    "\n",
    "      # Comment this to demonstrate the truncation condition.\n",
    "      if self.agent_pos[0] > 5 or self.agent_pos[0] < 0 or self.agent_pos[1] > 5 or self.agent_pos[1] < 0:\n",
    "        reward = -150\n",
    "        #print(\"bounding: \", self.agent_pos)\n",
    "      self.agent_pos = np.clip(self.agent_pos, 0, 5)\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      observation = self.state.flatten()\n",
    "\n",
    "\n",
    "      self.timestep += 1\n",
    "\n",
    "      # Condition to check for termination (episode is over)\n",
    "      terminated = True if self.timestep >= self.max_timesteps else False\n",
    "\n",
    "      # Condition to check if agent is traversing to a cell beyond the permitted cells\n",
    "      # This helps the agent to learn how to behave in a safe and predictable manner\n",
    "      truncated = True if np.all((np.asarray(self.agent_pos) >=0 ) & (np.asarray(self.agent_pos) <= 6)) else False\n",
    "      #print(self.agent_pos)\n",
    "\n",
    "      return observation, reward, terminated, self.package\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "      plt.title('Grid Environment')\n",
    "      plt.imshow(self.state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T22:22:15.355997Z",
     "start_time": "2024-11-02T22:22:15.201496Z"
    }
   },
   "execution_count": 179
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 64)\n",
    "        self.layer2 = nn.Linear(64, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        return self.layer2(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T21:20:59.441316Z",
     "start_time": "2024-11-02T21:20:59.433971Z"
    }
   },
   "execution_count": 129
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T22:36:31.342418Z",
     "start_time": "2024-11-02T22:36:31.325377Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, C, learning_rate):\n",
    "    # initialize environment\n",
    "    self.env = GridEnvironment()\n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = [None] * N\n",
    "    self.capacity = N\n",
    "    self.pointer = 0\n",
    "    self.policy_net = Net(2, 6).to(device)\n",
    "    self.target_net = Net(2, 6).to(device)\n",
    "\n",
    "    self.optimizer = optim.SGD(self.policy_net.parameters(), lr=learning_rate, momentum=0.9) \n",
    "    self.C = C\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, gamma, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    for i in range(episodes):\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      state = torch.tensor([0, 0],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "      terminated = False\n",
    "      pos = package = rewards = steps = 0\n",
    "      # Decay epsilon after every episode\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      self.env.reset()\n",
    "      while not terminated:\n",
    "        # Select action\n",
    "        action_type = action_function(eps, pos, package)\n",
    "\n",
    "        # Execute action and observe reward\n",
    "        position, reward, terminated, package = self.env.step(action_type)\n",
    "        pos = np.where(position == 50)[0][0]\n",
    "\n",
    "        # Format next state\n",
    "        next_state = torch.tensor([pos, package], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # # See greedy steps (Might be going back and forth)\n",
    "        s = state[0][0].item()\n",
    "        if greedy:\n",
    "            print(f\"{s} -> {pos}\")\n",
    "\n",
    "        # Add to total rewards for the episode\n",
    "        rewards += reward\n",
    "        # Encode action type for ease of use\n",
    "        action_type = torch.tensor([action_type], device=device, dtype=torch.int64)\n",
    "        # store transition in replay buffer\n",
    "        transition = state, action_type,  next_state, reward\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "        self.replay[self.pointer % self.capacity] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "        # When terminated store the last value found\n",
    "        if terminated:\n",
    "            transition = state, action_type,  None, reward\n",
    "            self.replay[self.pointer % self.capacity] = transition\n",
    "            self.pointer += 1\n",
    "            \n",
    "        batch_size = 128\n",
    "        # Run the replay function if there is enough transitions\n",
    "        if batch_size < self.pointer:\n",
    "            self.replay_function(gamma ** steps, batch_size)\n",
    "        \n",
    "        test = torch.tensor([0, 0],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Every C steps update the target function\n",
    "        if steps % self.C == 0:\n",
    "            pol = self.policy_net(test)\n",
    "            tar = self.target_net(test)\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            after = self.target_net(test)\n",
    "            # print(\"policy: \", pol)\n",
    "            # print(\"target: \", tar)\n",
    "            # print(\"after: \", after)\n",
    "            \n",
    "\n",
    "        steps += 1\n",
    "    \n",
    "      print(\"Episode: \", i, \" Reward: \", rewards)\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "  def warehouse_action(self, epsilon, pos, package):\n",
    "      if np.random.rand() < epsilon:\n",
    "        action_type = np.random.randint(6)\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor([pos, package], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "            action_type = self.policy_net(state).max(1).indices.item()\n",
    "      return action_type\n",
    "      \n",
    "  def replay_function(self, gamma, batch_size):\n",
    "    if self.pointer < self.capacity:\n",
    "        temp = self.replay[:self.pointer]\n",
    "        sample = random.sample(temp, k=batch_size)\n",
    "    else:\n",
    "        sample = random.sample(self.replay, k=batch_size)\n",
    "    Q_list = torch.tensor([] , device=device)\n",
    "    target_val = torch.tensor([] , device=device)\n",
    "    action_list = torch.tensor([], dtype=torch.int64, device=device)\n",
    "    for state, action, next_state, reward in sample:\n",
    "        if next_state is None:\n",
    "            Q_list = torch.cat((Q_list, self.policy_net(state)))\n",
    "            # Make an actions array\n",
    "            action_list = torch.cat((action_list, action), 0)\n",
    "\n",
    "            # Calculate updated Q value\n",
    "            Q_val = torch.tensor([reward], device=device)\n",
    "            # Add value to expected target list\n",
    "            target_val = torch.cat((target_val, Q_val))\n",
    "        else:\n",
    "            # Take entire Q row\n",
    "            #print(self.policy_net(state))\n",
    "            Q_list = torch.cat((Q_list, self.policy_net(state)))\n",
    "            #print(self.policy_net(state))\n",
    "            # Make an actions array\n",
    "            action_list = torch.cat((action_list, action), 0)\n",
    "            # Take max expected Q from the target network\n",
    "            max_expected = self.target_net(next_state).max(1).values\n",
    "            \n",
    "            # Calculate updated Q value\n",
    "            Q_val = torch.tensor([(max_expected * gamma) + reward], device=device)\n",
    "            # Add value to expected target list\n",
    "            target_val = torch.cat((target_val, Q_val))\n",
    "            # t = state[0][0].item()\n",
    "            # if t == 19:\n",
    "            #     print(self.policy_net(state)[0][4], Q_val)\n",
    "\n",
    "    # Apply the action list to get real expected Q values\n",
    "    selected_q_values = Q_list.gather(1, action_list.unsqueeze(1))\n",
    "\n",
    "    \n",
    "    loss_function = nn.MSELoss()\n",
    "    loss = loss_function(selected_q_values, target_val.unsqueeze(1))\n",
    "        \n",
    "    #backprop\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "              \n",
    "\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(\"pickles/\" + filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T21:21:03.828771Z",
     "start_time": "2024-11-02T21:21:03.816256Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints the reward per episode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - abs(int(min(reward_per_episode)) * (.2))\n",
    "    maxs = int(max(reward_per_episode)) + abs(int(max(reward_per_episode)) * (.3) )\n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def print_Qtable(policy):\n",
    "    for pos in range(36):\n",
    "        for package in range(2):\n",
    "            state = torch.tensor([pos, package],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "            table = policy(state).squeeze(1)[0]\n",
    "            ret = f\"({pos}, {package}):\\n0:{table[0].item()}, 1:{table[1].item()}, 2:{table[2].item()}, 3:{table[3].item()}, 4:{table[4].item()}, 5:{table[5].item()}\"\n",
    "            print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----picked up----\n",
      " 54\n",
      "Episode:  0  Reward:  -8385\n",
      "\n",
      "----picked up----\n",
      " 28\n",
      "Episode:  1  Reward:  -12854\n",
      "\n",
      "----picked up----\n",
      " 138\n",
      "Episode:  2  Reward:  -11514\n",
      "Episode:  3  Reward:  -11772\n",
      "Episode:  4  Reward:  -16280\n",
      "\n",
      "----picked up----\n",
      " 50\n",
      "Episode:  5  Reward:  -14100\n",
      "Episode:  6  Reward:  -15599\n",
      "Episode:  7  Reward:  -15845\n",
      "Episode:  8  Reward:  -15106\n",
      "Episode:  9  Reward:  -16546\n",
      "Episode:  10  Reward:  -15843\n",
      "Episode:  11  Reward:  -17242\n",
      "Episode:  12  Reward:  -19822\n",
      "Episode:  13  Reward:  -17386\n",
      "Episode:  14  Reward:  -20172\n",
      "Episode:  15  Reward:  -19172\n",
      "\n",
      "----picked up----\n",
      " 134\n",
      "Episode:  16  Reward:  -17580\n",
      "Episode:  17  Reward:  -17178\n",
      "Episode:  18  Reward:  -20160\n",
      "Episode:  19  Reward:  -17972\n",
      "Episode:  20  Reward:  -18215\n",
      "Episode:  21  Reward:  -15260\n",
      "Episode:  22  Reward:  -16952\n",
      "Episode:  23  Reward:  -15102\n",
      "Episode:  24  Reward:  -15261\n",
      "Episode:  25  Reward:  -16507\n",
      "Episode:  26  Reward:  -15701\n",
      "\n",
      "----picked up----\n",
      " 111\n",
      "Episode:  27  Reward:  -17504\n",
      "Episode:  28  Reward:  -15955\n",
      "Episode:  29  Reward:  -16402\n",
      "Episode:  30  Reward:  -15608\n",
      "Episode:  31  Reward:  -16509\n",
      "Episode:  32  Reward:  -17355\n",
      "Episode:  33  Reward:  -15657\n",
      "Episode:  34  Reward:  -14909\n",
      "Episode:  35  Reward:  -15556\n",
      "Episode:  36  Reward:  -15011\n",
      "Episode:  37  Reward:  -15506\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[193], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m discount \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.9\u001B[39m\n\u001B[0;32m     10\u001B[0m action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mwarehouse_action\n\u001B[1;32m---> 11\u001B[0m total_rewards \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mmax\u001B[39m(total_rewards))\n\u001B[0;32m     13\u001B[0m agent\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrpreisl_angustsa_assignment2_part2_dqn_gridworld.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[192], line 65\u001B[0m, in \u001B[0;36mDQN.train\u001B[1;34m(self, episodes, epsilon, gamma, action_function, greedy)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# Run the replay function if there is enough transitions\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_size \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpointer:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m test \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m],device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     69\u001B[0m \u001B[38;5;66;03m# Every C steps update the target function\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[192], line 125\u001B[0m, in \u001B[0;36mDQN.replay_function\u001B[1;34m(self, gamma, batch_size)\u001B[0m\n\u001B[0;32m    122\u001B[0m max_expected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_net(next_state)\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# Calculate updated Q value\u001B[39;00m\n\u001B[1;32m--> 125\u001B[0m Q_val \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_expected\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mreward\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# Add value to expected target list\u001B[39;00m\n\u001B[0;32m    127\u001B[0m target_val \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((target_val, Q_val))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "C = 5\n",
    "learning_rate = 0.01\n",
    "env = GridEnvironment()\n",
    "agent = DQN(N, C, learning_rate)\n",
    "\n",
    "episodes = 50\n",
    "epsilon = .9\n",
    "discount = .9\n",
    "action = agent.warehouse_action\n",
    "total_rewards = agent.train(episodes, epsilon, discount, action, False)\n",
    "print(\"Best reward: \", max(total_rewards))\n",
    "agent.save(\"drpreisl_angustsa_assignment2_part2_dqn_gridworld.pickle\")\n",
    "reward_print(total_rewards, episodes, \"grid world\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = agent.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"greedy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T22:39:11.562286Z",
     "start_time": "2024-11-02T22:36:33.338562Z"
    }
   },
   "execution_count": 193
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0):\n",
      "0:-2.542151689529419, 1:-38.29107666015625, 2:1.5270297527313232, 3:-70.99676513671875, 4:-99.9378662109375, 5:-149.4581756591797\n",
      "(0, 1):\n",
      "0:1.5410888195037842, 1:-29.07898712158203, 2:-4.85618782043457, 3:-74.52448272705078, 4:-96.48236846923828, 5:-144.6983184814453\n",
      "(1, 0):\n",
      "0:-1.21623694896698, 1:-27.564199447631836, 2:-1.0735175609588623, 3:-1.1042839288711548, 4:-100.11540985107422, 5:-137.6320037841797\n",
      "(1, 1):\n",
      "0:-2.492593288421631, 1:-2.4892325401306152, 2:-3.1471803188323975, 3:-2.493431806564331, 4:-100.08247375488281, 5:-131.34207153320312\n",
      "(2, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(2, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(3, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(3, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(4, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(4, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(5, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(5, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(6, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(6, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(7, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(7, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(8, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(8, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(9, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(9, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(10, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(10, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(11, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(11, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(12, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(12, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(13, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(13, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(14, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(14, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(15, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(15, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(16, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(16, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(17, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(17, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(18, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(18, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(19, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(19, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(20, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(20, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(21, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(21, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(22, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(22, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(23, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(23, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(24, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(24, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(25, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(25, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(26, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(26, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(27, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(27, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(28, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(28, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(29, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(29, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(30, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(30, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(31, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(31, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(32, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(32, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(33, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(33, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(34, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(34, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(35, 0):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n",
      "(35, 1):\n",
      "0:-2.4692890644073486, 1:-2.5058281421661377, 2:-3.1561686992645264, 3:-2.511659622192383, 4:-100.10491943359375, 5:-131.32034301757812\n"
     ]
    }
   ],
   "source": [
    "print_Qtable(agent.target_net)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T22:27:26.835135Z",
     "start_time": "2024-11-02T22:27:26.821305Z"
    }
   },
   "execution_count": 181
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ep_decay(.96, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-02T04:39:15.996111Z",
     "start_time": "2024-11-02T04:39:15.996111Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
