{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T23:16:43.629657Z",
     "start_time": "2024-11-01T23:16:43.614731Z"
    }
   },
   "execution_count": 210
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "\n",
    "      self.observation_space = spaces.Discrete(36)\n",
    "      self.action_space = spaces.Discrete(6)\n",
    "      self.max_timesteps = 150\n",
    "\n",
    "      self.timestep = 0\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.goal_pos = [5, 5]\n",
    "      self.package_pos = [3, 1]\n",
    "      self.state = np.zeros((6,6))\n",
    "      #Shelves\n",
    "      self.wall_1 = [3,2]\n",
    "      self.wall_2 = [4,2]\n",
    "      self.wall_3 = [5,2]\n",
    "      self.wall_4 = [1,4]\n",
    "      self.wall_5 = [1,5]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "\n",
    "      self.package = 0\n",
    "      self.pickup = 0\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.timestep = 0\n",
    "      observation = self.state.flatten()\n",
    "      self.package =0\n",
    "      info = {}\n",
    "\n",
    "      return observation, info\n",
    "\n",
    "    # Step function: Contains the implementation for what happens when an\n",
    "    # agent takes a step in the environment.\n",
    "    def step(self, action):\n",
    "      prev = self.agent_pos.copy()\n",
    "      if action == 0: #down\n",
    "        self.agent_pos[0] += 1\n",
    "      if action == 1: #up\n",
    "        self.agent_pos[0] -= 1\n",
    "      if action == 2: #right\n",
    "        self.agent_pos[1] += 1\n",
    "      if action == 3: #left\n",
    "        self.agent_pos[1] -= 1\n",
    "\n",
    "      reward = -1\n",
    "      if action == 4: # Pick up\n",
    "        if np.array_equal(self.agent_pos, self.package_pos) and self.package == 0: #Picked up, in right location\n",
    "          self.package = 1\n",
    "          reward = 100\n",
    "          print(\"\\n----picked up----\\n\", self.timestep)\n",
    "          #self.pickup += 1\n",
    "        elif self.package: #Picked up while holding a package\n",
    "          reward = -100\n",
    "        else: # Picked up in wrong location\n",
    "          reward = -10\n",
    "\n",
    "      if action == 5:\n",
    "        #Drop off\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and self.package == 1: #Dropped off in right location\n",
    "          reward = 100\n",
    "          print(\"----dropped off----\", self.timestep)\n",
    "          self.package = 0\n",
    "          self.timestep = self.max_timesteps\n",
    "        elif self.package == 0: #Dropped off without holding a package\n",
    "          reward = -100\n",
    "        else: #dropped off in wrong location\n",
    "          reward = -10\n",
    "\n",
    "      if np.array_equal(self.agent_pos, self.wall_1) or np.array_equal(self.agent_pos, self.wall_2) or np.array_equal(self.agent_pos, self.wall_3) or np.array_equal(self.agent_pos, self.wall_4) or np.array_equal(self.agent_pos, self.wall_5):\n",
    "        reward = -20\n",
    "        self.agent_pos = prev\n",
    "        #print(\"wall\")\n",
    "\n",
    "      # Comment this to demonstrate the truncation condition.\n",
    "      if self.agent_pos[0] > 5 or self.agent_pos[0] < 0 or self.agent_pos[1] > 5 or self.agent_pos[1] < 0:\n",
    "        reward = -25\n",
    "        #print(\"bounding: \", self.agent_pos)\n",
    "      self.agent_pos = np.clip(self.agent_pos, 0, 5)\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      observation = self.state.flatten()\n",
    "\n",
    "\n",
    "      self.timestep += 1\n",
    "\n",
    "      # Condition to check for termination (episode is over)\n",
    "      terminated = True if self.timestep >= self.max_timesteps else False\n",
    "\n",
    "      # Condition to check if agent is traversing to a cell beyond the permitted cells\n",
    "      # This helps the agent to learn how to behave in a safe and predictable manner\n",
    "      truncated = True if np.all((np.asarray(self.agent_pos) >=0 ) & (np.asarray(self.agent_pos) <= 6)) else False\n",
    "      #print(self.agent_pos)\n",
    "\n",
    "      return observation, reward, terminated, self.package\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "      plt.title('Grid Environment')\n",
    "      plt.imshow(self.state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T23:16:45.341175Z",
     "start_time": "2024-11-01T23:16:45.322152Z"
    }
   },
   "execution_count": 211
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 144)\n",
    "        self.layer2 = nn.Linear(144, 144)\n",
    "        self.layer3 = nn.Linear(144, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T23:16:46.886929Z",
     "start_time": "2024-11-01T23:16:46.877151Z"
    }
   },
   "execution_count": 212
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T23:18:56.740864Z",
     "start_time": "2024-11-01T23:18:56.728263Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, C):\n",
    "    # initialize environment\n",
    "    self.env = GridEnvironment()\n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = []\n",
    "    self.capacity = N\n",
    "    self.pointer = 0\n",
    "    self.policy_net = Net(2, 6).to(device)\n",
    "    self.target_net = Net(2, 6).to(device)\n",
    "\n",
    "    self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "    self.C = C\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    for i in range(episodes):\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      state = torch.tensor([0, 0],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "      terminated = False\n",
    "      pos = package = rewards = steps = 0\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      self.env.reset()\n",
    "      while not terminated:\n",
    "        # Select action\n",
    "        action_type = action_function(eps, pos, package)\n",
    "\n",
    "        # Execute action and observe reward\n",
    "        position, reward, terminated, package = self.env.step(action_type)\n",
    "        pos = np.where(position == 50)[0][0]\n",
    "\n",
    "        # Format next state\n",
    "        next_state = torch.tensor([pos, package], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # # See greedy steps (Might be going back and forth)\n",
    "        # s = state[0][0].item()\n",
    "        # if greedy:\n",
    "        #     print(f\"{s} -> {pos}\")\n",
    "\n",
    "        # Add to total rewards for the episode\n",
    "        rewards += reward\n",
    "        # Encode action type for ease of use\n",
    "        action_type = torch.tensor([action_type], device=device, dtype=torch.int64)\n",
    "        # store transition in replay buffer\n",
    "        transition = state, action_type,  next_state, reward\n",
    "        state = next_state\n",
    "\n",
    "        if self.pointer < self.capacity:\n",
    "            self.replay.append(transition)\n",
    "        else:\n",
    "            self.replay[self.pointer % self.capacity] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "        # When terminated store the last value found\n",
    "        if terminated:\n",
    "            transition = state, action_type,  None, reward\n",
    "            if self.pointer < self.capacity:\n",
    "                self.replay.append(transition)\n",
    "            else:\n",
    "                self.replay[self.pointer % self.capacity] = transition\n",
    "            self.pointer += 1\n",
    "\n",
    "        # Run the replay function\n",
    "        self.replay_function(discount ** steps)\n",
    "        # Every C steps update the target function\n",
    "        if steps % self.C == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        steps += 1\n",
    "    \n",
    "      # Decay epsilon after every episode\n",
    "      print(\"Episode: \", i)\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "  def warehouse_action(self, epsilon, pos, package):\n",
    "      if np.random.rand() < epsilon:\n",
    "        action_type = np.random.randint(6)\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor([pos, package], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "            action_type = self.policy_net(state).max(1).indices.item()\n",
    "      return action_type\n",
    "      \n",
    "  def replay_function(self, discount):\n",
    "    BATCH_SIZE = 128\n",
    "    if len(self.replay) < BATCH_SIZE:\n",
    "        return\n",
    "    else:\n",
    "        sample = random.sample(self.replay, k=BATCH_SIZE)\n",
    "        Q_list = torch.tensor([] , device=device)\n",
    "        target_list = torch.tensor([] , device=device)\n",
    "        action_list = torch.tensor([], dtype=torch.int64, device=device)\n",
    "        for state, action, next_state, reward in sample:\n",
    "            if next_state is None:\n",
    "                Q_list = torch.cat((Q_list, self.policy_net(state)))\n",
    "                # Make an actions array\n",
    "                action_list = torch.cat((action_list, action), 0)\n",
    "\n",
    "                # Calculate updated Q value\n",
    "                Q_val = torch.tensor([reward], device=device)\n",
    "                # Add value to expected target list\n",
    "                target_list = torch.cat((target_list, Q_val))\n",
    "            else:\n",
    "                    # Take entire Q row\n",
    "                    Q_list = torch.cat((Q_list, self.policy_net(state)))\n",
    "                    # Make an actions array\n",
    "                    action_list = torch.cat((action_list, action), 0)\n",
    "                    # Take max expected Q from the target network\n",
    "                    max_expected = self.target_net(next_state).max(1).values\n",
    "\n",
    "                    # Calculate updated Q value\n",
    "                    Q_val = torch.tensor([(max_expected * discount) + reward], device=device)\n",
    "                    # Add value to expected target list\n",
    "                    target_list = torch.cat((target_list, Q_val))\n",
    "                    # t = state[0][0].item()\n",
    "                    # if t == 19:\n",
    "                    #     print(self.policy_net(state)[0][4], Q_val)\n",
    "\n",
    "\n",
    "        # Apply the action list to get real expected Q values\n",
    "        selected_q_values = Q_list.gather(1, action_list.unsqueeze(1))\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        \n",
    "        loss = criterion(selected_q_values, target_list.unsqueeze(1))\n",
    "        #backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "              \n",
    "\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(\"pickles/\" + filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T23:16:53.020514Z",
     "start_time": "2024-11-01T23:16:53.008624Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints the reward per episode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - abs(int(min(reward_per_episode)) * (.2))\n",
    "    maxs = int(max(reward_per_episode)) + abs(int(max(reward_per_episode)) * (.3) )\n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def print_Qtable(policy):\n",
    "    for pos in range(36):\n",
    "        for package in range(2):\n",
    "            state = torch.tensor([pos, package],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "            table = policy(state).squeeze(1)[0]\n",
    "            ret = f\"({pos}, {package}):\\n0:{table[0].item()}, 1:{table[1].item()}, 2:{table[2].item()}, 3:{table[3].item()}, 4:{table[4].item()}, 5:{table[5].item()}\"\n",
    "            print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Episode:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[222], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m discount \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.99\u001B[39m\n\u001B[0;32m      9\u001B[0m action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mwarehouse_action\n\u001B[1;32m---> 10\u001B[0m total_rewards \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mmax\u001B[39m(total_rewards))\n\u001B[0;32m     12\u001B[0m agent\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrpreisl_angustsa_assignment2_part2_dqn_gridworld.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[221], line 67\u001B[0m, in \u001B[0;36mDQN.train\u001B[1;34m(self, episodes, epsilon, discount, action_function, greedy)\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpointer \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# Run the replay function\u001B[39;00m\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# Every C steps update the target function\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m steps \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[221], line 132\u001B[0m, in \u001B[0;36mDQN.replay_function\u001B[1;34m(self, discount)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;66;03m#backprop\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 132\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "C = 10\n",
    "env = GridEnvironment()\n",
    "agent = DQN(N, C)\n",
    "\n",
    "episodes = 50\n",
    "epsilon = .93\n",
    "discount = .99\n",
    "action = agent.warehouse_action\n",
    "total_rewards = agent.train(episodes, epsilon, discount, action, False)\n",
    "print(\"Best reward: \", max(total_rewards))\n",
    "agent.save(\"drpreisl_angustsa_assignment2_part2_dqn_gridworld.pickle\")\n",
    "reward_print(total_rewards, episodes, \"grid world\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = agent.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"greedy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T23:19:27.703929Z",
     "start_time": "2024-11-01T23:18:59.165377Z"
    }
   },
   "execution_count": 222
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0):\n",
      "0:-4.256524085998535, 1:-23.100841522216797, 2:-1.3640106916427612, 3:-1.8137857913970947, 4:-11.875450134277344, 5:-74.26509094238281\n",
      "(0, 1):\n",
      "0:-3.508329391479492, 1:-19.756092071533203, 2:-1.2066541910171509, 3:-1.630937933921814, 4:-10.416214942932129, 5:-63.05467224121094\n",
      "(1, 0):\n",
      "0:-4.699248790740967, 1:-23.54338264465332, 2:-1.5114866495132446, 3:-1.456565022468567, 4:-12.545512199401855, 5:-81.1836166381836\n",
      "(1, 1):\n",
      "0:-3.783578872680664, 1:-19.8686580657959, 2:-1.3318525552749634, 3:-1.4889720678329468, 4:-10.946609497070312, 5:-68.13060760498047\n",
      "(2, 0):\n",
      "0:-4.6530327796936035, 1:-21.555984497070312, 2:-1.647426962852478, 3:-1.4204210042953491, 4:-12.260791778564453, 5:-82.24165344238281\n",
      "(2, 1):\n",
      "0:-3.7226483821868896, 1:-17.985553741455078, 2:-1.4357489347457886, 3:-1.4665213823318481, 4:-10.66545581817627, 5:-69.0163345336914\n",
      "(3, 0):\n",
      "0:-4.4883599281311035, 1:-19.19803810119629, 2:-1.7354801893234253, 3:-1.5489133596420288, 4:-11.802240371704102, 5:-82.0605697631836\n",
      "(3, 1):\n",
      "0:-3.51955509185791, 1:-15.580613136291504, 2:-1.5176563262939453, 3:-1.5214825868606567, 4:-10.131261825561523, 5:-68.4452133178711\n",
      "(4, 0):\n",
      "0:-4.310585975646973, 1:-16.88311195373535, 2:-1.8175116777420044, 3:-1.6859145164489746, 4:-11.355268478393555, 5:-81.85868072509766\n",
      "(4, 1):\n",
      "0:-3.333653688430786, 1:-13.340606689453125, 2:-1.5881096124649048, 3:-1.6356816291809082, 4:-9.671075820922852, 5:-68.33625030517578\n",
      "(5, 0):\n",
      "0:-4.132275104522705, 1:-14.681564331054688, 2:-1.8941258192062378, 3:-1.8292001485824585, 4:-10.939695358276367, 5:-81.9371109008789\n",
      "(5, 1):\n",
      "0:-3.11566424369812, 1:-11.109114646911621, 2:-1.6294814348220825, 3:-1.7803781032562256, 4:-9.204891204833984, 5:-68.16728210449219\n",
      "(6, 0):\n",
      "0:-3.8826682567596436, 1:-12.63651180267334, 2:-1.932538628578186, 3:-1.9825881719589233, 4:-10.542267799377441, 5:-82.14476776123047\n",
      "(6, 1):\n",
      "0:-2.8697669506073, 1:-9.134381294250488, 2:-1.6365939378738403, 3:-1.9259990453720093, 4:-8.830648422241211, 5:-68.47116088867188\n",
      "(7, 0):\n",
      "0:-3.590839385986328, 1:-10.858855247497559, 2:-1.937183141708374, 3:-2.1140084266662598, 4:-10.244935035705566, 5:-82.53702545166016\n",
      "(7, 1):\n",
      "0:-2.6027615070343018, 1:-7.46409797668457, 2:-1.6190415620803833, 3:-2.0662920475006104, 4:-8.561765670776367, 5:-68.9647216796875\n",
      "(8, 0):\n",
      "0:-3.2963740825653076, 1:-9.218583106994629, 2:-1.9497512578964233, 3:-2.230771541595459, 4:-9.975805282592773, 5:-83.03291320800781\n",
      "(8, 1):\n",
      "0:-2.3318941593170166, 1:-6.321944713592529, 2:-1.61201810836792, 3:-2.1585206985473633, 4:-8.449043273925781, 5:-69.97332763671875\n",
      "(9, 0):\n",
      "0:-3.0188026428222656, 1:-7.9870781898498535, 2:-1.9400221109390259, 3:-2.3238606452941895, 4:-9.839879035949707, 5:-83.9322738647461\n",
      "(9, 1):\n",
      "0:-2.1337640285491943, 1:-5.5752949714660645, 2:-1.6192554235458374, 3:-2.257664680480957, 4:-8.489912033081055, 5:-71.49021911621094\n",
      "(10, 0):\n",
      "0:-2.8227033615112305, 1:-7.1971869468688965, 2:-1.940040946006775, 3:-2.411426544189453, 4:-9.85837173461914, 5:-85.27511596679688\n",
      "(10, 1):\n",
      "0:-1.9511398077011108, 1:-4.926403522491455, 2:-1.6267751455307007, 3:-2.3838796615600586, 4:-8.56226921081543, 5:-73.15062713623047\n",
      "(11, 0):\n",
      "0:-2.633500099182129, 1:-6.4459099769592285, 2:-1.9438904523849487, 3:-2.5083014965057373, 4:-9.893253326416016, 5:-86.66108703613281\n",
      "(11, 1):\n",
      "0:-1.776130199432373, 1:-4.368257999420166, 2:-1.645936131477356, 3:-2.525533676147461, 4:-8.66157341003418, 5:-74.9935073852539\n",
      "(12, 0):\n",
      "0:-2.4552693367004395, 1:-5.741420269012451, 2:-1.9488917589187622, 3:-2.6252167224884033, 4:-9.94774055480957, 5:-88.11553955078125\n",
      "(12, 1):\n",
      "0:-1.606423020362854, 1:-3.8193418979644775, 2:-1.6674987077713013, 3:-2.6684908866882324, 4:-8.765130996704102, 5:-76.86410522460938\n",
      "(13, 0):\n",
      "0:-2.2773308753967285, 1:-5.140264987945557, 2:-1.9642611742019653, 3:-2.76364803314209, 4:-10.025290489196777, 5:-89.74219512939453\n",
      "(13, 1):\n",
      "0:-1.4710129499435425, 1:-3.358167886734009, 2:-1.704494833946228, 3:-2.825375556945801, 4:-8.928914070129395, 5:-79.10462951660156\n",
      "(14, 0):\n",
      "0:-2.100132942199707, 1:-4.571316719055176, 2:-1.9851311445236206, 3:-2.9114527702331543, 4:-10.119397163391113, 5:-91.46893310546875\n",
      "(14, 1):\n",
      "0:-1.3431854248046875, 1:-2.9462945461273193, 2:-1.7499265670776367, 3:-2.9929943084716797, 4:-9.125704765319824, 5:-81.53157806396484\n",
      "(15, 0):\n",
      "0:-1.9170411825180054, 1:-4.013863563537598, 2:-2.0109612941741943, 3:-3.064802885055542, 4:-10.218085289001465, 5:-93.20829772949219\n",
      "(15, 1):\n",
      "0:-1.224537968635559, 1:-2.5623743534088135, 2:-1.797738790512085, 3:-3.1653950214385986, 4:-9.340841293334961, 5:-84.09278869628906\n",
      "(16, 0):\n",
      "0:-1.7622065544128418, 1:-3.5287816524505615, 2:-2.0446524620056152, 3:-3.230496883392334, 4:-10.366241455078125, 5:-95.25672149658203\n",
      "(16, 1):\n",
      "0:-1.1058903932571411, 1:-2.1784536838531494, 2:-1.8455511331558228, 3:-3.337796449661255, 4:-9.555977821350098, 5:-86.65398406982422\n",
      "(17, 0):\n",
      "0:-1.6209113597869873, 1:-3.081408977508545, 2:-2.0835976600646973, 3:-3.399076223373413, 4:-10.538355827331543, 5:-97.48145294189453\n",
      "(17, 1):\n",
      "0:-0.9875892400741577, 1:-1.794811487197876, 2:-1.8936649560928345, 3:-3.5104105472564697, 4:-9.771095275878906, 5:-89.21739196777344\n",
      "(18, 0):\n",
      "0:-1.4938629865646362, 1:-2.675625801086426, 2:-2.1290385723114014, 3:-3.569840431213379, 4:-10.7366943359375, 5:-99.91756439208984\n",
      "(18, 1):\n",
      "0:-0.8696314096450806, 1:-1.4114490747451782, 2:-1.9420784711837769, 3:-3.6832361221313477, 4:-9.986193656921387, 5:-91.7829818725586\n",
      "(19, 0):\n",
      "0:-1.380251407623291, 1:-2.304265022277832, 2:-2.1800243854522705, 3:-3.7439382076263428, 4:-10.959896087646484, 5:-102.55220031738281\n",
      "(19, 1):\n",
      "0:-0.7516733407974243, 1:-1.0280838012695312, 2:-1.9904924631118774, 3:-3.8560619354248047, 4:-10.20129108428955, 5:-94.34856414794922\n",
      "(20, 0):\n",
      "0:-1.2675902843475342, 1:-1.935134768486023, 2:-2.231231689453125, 3:-3.918252944946289, 4:-11.184966087341309, 5:-105.2002182006836\n",
      "(20, 1):\n",
      "0:-0.6376768350601196, 1:-0.6544021368026733, 2:-2.04082989692688, 3:-4.030200958251953, 4:-10.423284530639648, 5:-96.97168731689453\n",
      "(21, 0):\n",
      "0:-1.154929518699646, 1:-1.5660040378570557, 2:-2.282440185546875, 3:-4.09256649017334, 4:-11.41003704071045, 5:-107.84823608398438\n",
      "(21, 1):\n",
      "0:-0.525453507900238, 1:-0.2853739559650421, 2:-2.0920250415802, 3:-4.204521656036377, 4:-10.648192405700684, 5:-99.61980438232422\n",
      "(22, 0):\n",
      "0:-1.0422685146331787, 1:-1.1968733072280884, 2:-2.3336472511291504, 3:-4.266880989074707, 4:-11.635106086730957, 5:-110.49626159667969\n",
      "(22, 1):\n",
      "0:-0.41323113441467285, 1:0.08365395665168762, 2:-2.1432206630706787, 3:-4.378842830657959, 4:-10.8731050491333, 5:-102.26795196533203\n",
      "(23, 0):\n",
      "0:-0.9296083450317383, 1:-0.8277425765991211, 2:-2.3848557472229004, 3:-4.441195011138916, 4:-11.860177993774414, 5:-113.14427185058594\n",
      "(23, 1):\n",
      "0:-0.30100786685943604, 1:0.45268210768699646, 2:-2.19441556930542, 3:-4.553163528442383, 4:-11.098015785217285, 5:-104.91607666015625\n",
      "(24, 0):\n",
      "0:-0.8169469833374023, 1:-0.45861247181892395, 2:-2.436063051223755, 3:-4.615509510040283, 4:-12.085246086120605, 5:-115.79229736328125\n",
      "(24, 1):\n",
      "0:-0.18878519535064697, 1:0.8217097520828247, 2:-2.2456109523773193, 3:-4.727484226226807, 4:-11.32292652130127, 5:-107.56419372558594\n",
      "(25, 0):\n",
      "0:-0.7042861580848694, 1:-0.08948180079460144, 2:-2.4872705936431885, 3:-4.789823055267334, 4:-12.310315132141113, 5:-118.4403076171875\n",
      "(25, 1):\n",
      "0:-0.07656241208314896, 1:1.1907368898391724, 2:-2.296807050704956, 3:-4.901804447174072, 4:-11.54783821105957, 5:-110.21233367919922\n",
      "(26, 0):\n",
      "0:-0.5916253328323364, 1:0.2796503007411957, 2:-2.5384786128997803, 3:-4.964137554168701, 4:-12.535387992858887, 5:-121.08833312988281\n",
      "(26, 1):\n",
      "0:0.035660140216350555, 1:1.5597647428512573, 2:-2.3480021953582764, 3:-5.076125144958496, 4:-11.772749900817871, 5:-112.8604507446289\n",
      "(27, 0):\n",
      "0:-0.47896456718444824, 1:0.6487783193588257, 2:-2.589686155319214, 3:-5.138453960418701, 4:-12.760456085205078, 5:-123.736328125\n",
      "(27, 1):\n",
      "0:0.14788353443145752, 1:1.9287925958633423, 2:-2.3991973400115967, 3:-5.25044584274292, 4:-11.997662544250488, 5:-115.50859069824219\n",
      "(28, 0):\n",
      "0:-0.36630403995513916, 1:1.0179091691970825, 2:-2.6408941745758057, 3:-5.312767028808594, 4:-12.985526084899902, 5:-126.38436126708984\n",
      "(28, 1):\n",
      "0:0.26004624366760254, 1:2.2972633838653564, 2:-2.4503366947174072, 3:-5.424501895904541, 4:-12.222676277160645, 5:-118.1585693359375\n",
      "(29, 0):\n",
      "0:-0.2536433935165405, 1:1.387039065361023, 2:-2.6921021938323975, 3:-5.487081527709961, 4:-13.210597038269043, 5:-129.0323944091797\n",
      "(29, 1):\n",
      "0:0.3710007667541504, 1:2.654496431350708, 2:-2.5003459453582764, 3:-5.5932087898254395, 4:-12.449745178222656, 5:-120.84577941894531\n",
      "(30, 0):\n",
      "0:-0.14206767082214355, 1:1.7460793256759644, 2:-2.7422938346862793, 3:-5.656592845916748, 4:-13.43751335144043, 5:-131.71383666992188\n",
      "(30, 1):\n",
      "0:0.4757305383682251, 1:3.008369207382202, 2:-2.5500199794769287, 3:-5.763508319854736, 4:-12.6849365234375, 5:-123.55927276611328\n",
      "(31, 0):\n",
      "0:-0.030676253139972687, 1:2.1034116744995117, 2:-2.7923169136047363, 3:-5.8252949714660645, 4:-13.664739608764648, 5:-134.4009552001953\n",
      "(31, 1):\n",
      "0:0.5798757076263428, 1:3.361926317214966, 2:-2.5996618270874023, 3:-5.933955669403076, 4:-12.920888900756836, 5:-126.27523803710938\n",
      "(32, 0):\n",
      "0:0.08071660250425339, 1:2.4607484340667725, 2:-2.8423383235931396, 3:-5.993996620178223, 4:-13.89197063446045, 5:-137.0880584716797\n",
      "(32, 1):\n",
      "0:0.6840206384658813, 1:3.7154805660247803, 2:-2.6493046283721924, 3:-6.104404926300049, 4:-13.156843185424805, 5:-128.9912109375\n",
      "(33, 0):\n",
      "0:0.1921086311340332, 1:2.818084239959717, 2:-2.8923609256744385, 3:-6.1626973152160645, 4:-14.119197845458984, 5:-139.77516174316406\n",
      "(33, 1):\n",
      "0:0.7881665229797363, 1:4.069039344787598, 2:-2.6989471912384033, 3:-6.2748541831970215, 4:-13.392799377441406, 5:-131.70718383789062\n",
      "(34, 0):\n",
      "0:0.3035011291503906, 1:3.175419807434082, 2:-2.9423818588256836, 3:-6.331399440765381, 4:-14.34642505645752, 5:-142.46224975585938\n",
      "(34, 1):\n",
      "0:0.8922995328903198, 1:4.422770023345947, 2:-2.748520612716675, 3:-6.445274353027344, 4:-13.628738403320312, 5:-134.4231719970703\n",
      "(35, 0):\n",
      "0:0.408724308013916, 1:3.529423713684082, 2:-2.9920713901519775, 3:-6.501678466796875, 4:-14.581701278686523, 5:-145.17539978027344\n",
      "(35, 1):\n",
      "0:0.9964320659637451, 1:4.776546955108643, 2:-2.798077344894409, 3:-6.615687847137451, 4:-13.86467456817627, 5:-137.13914489746094\n"
     ]
    }
   ],
   "source": [
    "print_Qtable(agent.policy_net)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T05:33:15.428809Z",
     "start_time": "2024-11-01T05:33:15.255776Z"
    }
   },
   "execution_count": 418
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 -> 6\n",
      "6.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 2\n",
      "2.0 -> 1\n",
      "1.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 9\n",
      "9.0 -> 15\n",
      "15.0 -> 9\n",
      "9.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 9\n",
      "9.0 -> 9\n",
      "9.0 -> 9\n",
      "9.0 -> 15\n",
      "15.0 -> 9\n",
      "9.0 -> 15\n",
      "15.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 28\n",
      "28.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 16\n",
      "16.0 -> 17\n",
      "17.0 -> 23\n",
      "23.0 -> 17\n",
      "17.0 -> 17\n",
      "17.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 28\n",
      "28.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 28\n",
      "28.0 -> 22\n",
      "22.0 -> 28\n",
      "28.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 16\n",
      "Episode:  0\n",
      "0.0 -> 6\n",
      "6.0 -> 12\n",
      "12.0 -> 12\n",
      "12.0 -> 6\n",
      "6.0 -> 7\n",
      "7.0 -> 13\n",
      "13.0 -> 14\n",
      "14.0 -> 8\n",
      "8.0 -> 9\n",
      "9.0 -> 15\n",
      "15.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 22\n",
      "22.0 -> 16\n",
      "16.0 -> 16\n",
      "16.0 -> 15\n",
      "15.0 -> 9\n",
      "9.0 -> 9\n",
      "9.0 -> 15\n",
      "15.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 18\n",
      "18.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "Episode:  1\n",
      "0.0 -> 1\n",
      "1.0 -> 7\n",
      "7.0 -> 6\n",
      "6.0 -> 12\n",
      "12.0 -> 12\n",
      "12.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 18\n",
      "18.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 19\n",
      "19.0 -> 13\n",
      "13.0 -> 12\n",
      "12.0 -> 6\n",
      "6.0 -> 12\n",
      "12.0 -> 18\n",
      "18.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 24\n",
      "24.0 -> 18\n",
      "18.0 -> 12\n",
      "12.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 2\n",
      "2.0 -> 1\n",
      "1.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 13\n",
      "13.0 -> 7\n",
      "7.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 8\n",
      "8.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 14\n",
      "14.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 27\n",
      "27.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n",
      "15.0 -> 21\n",
      "21.0 -> 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[428], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m total_rewards \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m reward_print(total_rewards, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrid world\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[426], line 54\u001B[0m, in \u001B[0;36mDQN.train\u001B[1;34m(self, episodes, epsilon, discount, action_function, greedy)\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpointer \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcapacity] \u001B[38;5;241m=\u001B[39m transition\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpointer \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m steps \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_net\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_net\u001B[38;5;241m.\u001B[39mstate_dict())\n",
      "Cell \u001B[1;32mIn[426], line 88\u001B[0m, in \u001B[0;36mDQN.replay_function\u001B[1;34m(self, discount)\u001B[0m\n\u001B[0;32m     85\u001B[0m     target \u001B[38;5;241m=\u001B[39m reward\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     87\u001B[0m         \u001B[38;5;66;03m# Take entire Q row\u001B[39;00m\n\u001B[1;32m---> 88\u001B[0m         Q_list \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((Q_list, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     89\u001B[0m         \u001B[38;5;66;03m# Make an actions array\u001B[39;00m\n\u001B[0;32m     90\u001B[0m         action_list \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((action_list, action), \u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[4], line 11\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m#print(x)\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x))\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:1500\u001B[0m, in \u001B[0;36mrelu\u001B[1;34m(input, inplace)\u001B[0m\n\u001B[0;32m   1498\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m   1499\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1500\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "total_rewards = agent.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"grid world\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
