{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:51.775833Z",
     "start_time": "2024-10-31T19:01:51.771501Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:55.134415Z",
     "start_time": "2024-10-31T19:01:55.130999Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:14:53.327192Z",
     "start_time": "2024-10-31T19:14:53.315948Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, env):\n",
    "    # initialize environment\n",
    "    self.env = env\n",
    "    state, info = self.env.reset()\n",
    "    # initialize replay memory to capacity N\n",
    "    self.pointer = 0\n",
    "    self.policy_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    self.target_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    \n",
    "    self.optimizer = optim.AdamW(self.policy_net.parameters(),amsgrad=True ) #auto learning rate\n",
    "    # big replay memory:\n",
    "    self.size = N\n",
    "    self.state_mem = torch.zeros((N, self.env.observation_space.shape[0]), dtype=torch.float32, device=device)\n",
    "    self.next_state_mem = torch.zeros((self.size, self.env.observation_space.shape[0]), dtype=torch.float32, device=device)\n",
    "    self.action_mem = torch.zeros(self.size, dtype=torch.int64, device=device)\n",
    "    self.reward_mem = torch.zeros(self.size, dtype=torch.float32, device=device)\n",
    "    self.done_mem = torch.zeros(self.size, dtype=torch.float32, device=device)\n",
    "    self.pointer = 0\n",
    "\n",
    "  def append(self, state, action, reward, next_state, done):\n",
    "    i = self.pointer % self.size #get index\n",
    "    self.state_mem[i] = state\n",
    "    self.next_state_mem[i] = next_state\n",
    "    self.reward_mem[i] = reward \n",
    "    self.done_mem[i] = 1 - int(done)\n",
    "    self.action_mem[i] = action\n",
    "    self.pointer +=1\n",
    "    \n",
    "  def sample(self,batch):\n",
    "    mem = min(self.pointer,self.size) # get range to choose mem from\n",
    "    batch = np.random.choice(mem,batch) # choose random indices\n",
    "    states = self.state_mem[batch]\n",
    "    next_states = self.next_state_mem[batch]\n",
    "    actions = self.action_mem[batch]\n",
    "    rewards = self.reward_mem[batch]\n",
    "    done = self.done_mem[batch]\n",
    "    return states, actions, rewards, next_states, done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    # TAU = .0004\n",
    "    for i in range(episodes):\n",
    "      if i % 5 == 0:\n",
    "         self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      seq  = [None , None]\n",
    "      state, info = self.env.reset()\n",
    "      seq[0] = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "      done = False\n",
    "      rewards = 0\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      while not done:\n",
    "        # Select action\n",
    "        action_type = action_function(seq[0], eps)\n",
    "        observation, reward, terminated, truncated, _ = self.env.step(action_type.item())\n",
    "        state = torch.tensor(observation, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        # Set sequence\n",
    "        seq[1] = state\n",
    "        self.append(state=seq[0], action=action_type, reward=reward, next_state=seq[1], done=terminated)\n",
    "        if terminated:\n",
    "           seq[1] = None\n",
    "        rewards += reward\n",
    "        # store transition in replay buffer\n",
    "        seq[0] = state\n",
    "        \n",
    "        self.r(discount)\n",
    "        done = truncated or terminated\n",
    "      # Decay epsilon after every episode\n",
    "      # epsilon *= epsilon\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "      \n",
    "  def action(self, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_type = self.env.action_space.sample()\n",
    "    else:\n",
    "        # select max(Q)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).max(1).indices.view(1, 1) \n",
    "    return torch.tensor([[action_type]], device=device, dtype=torch.long)\n",
    "        \n",
    "  def r(self,discount):\n",
    "      BATCH_SIZE = 256\n",
    "      if self.pointer < BATCH_SIZE:\n",
    "          return\n",
    "      # Sample a batch from replay memory\n",
    "      states, actions, rewards, next_states, dones = self.sample(BATCH_SIZE)\n",
    "      \n",
    "      # Convert to tensors\n",
    "      state_batch = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "      action_batch = torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(1)\n",
    "      reward_batch = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "      next_state_batch = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "      done_mask = torch.tensor(dones, dtype=torch.float32, device=device) # zeros out terminated\n",
    "      \n",
    "      # Calculate state-action values\n",
    "      state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "      \n",
    "      # Calculate next state values\n",
    "      next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "      with torch.no_grad():\n",
    "          next_state_values[done_mask.bool()] = self.target_net(next_state_batch[done_mask.bool()]).max(1).values\n",
    "      \n",
    "      # Expected Q values\n",
    "      expected_state_action_values = (next_state_values * discount) + reward_batch\n",
    "      \n",
    "      # Compute loss\n",
    "      criterion = nn.SmoothL1Loss()\n",
    "      loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "      \n",
    "      # Optimize the model\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "  \n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net.state_dict(), file,protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:19:10.107111Z",
     "start_time": "2024-10-31T19:19:10.101012Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prints the reward per epsisode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - int(min(reward_per_episode)) * (.15)\n",
    "    maxs = int(max(reward_per_episode)) + int(max(reward_per_episode)) * (.3) \n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T21:41:36.703999Z",
     "start_time": "2024-10-31T21:41:36.241784Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_42504\\1473573199.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_batch = torch.tensor(states, dtype=torch.float32, device=device)\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_42504\\1473573199.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_batch = torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(1)\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_42504\\1473573199.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reward_batch = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_42504\\1473573199.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state_batch = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_42504\\1473573199.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  done_mask = torch.tensor(dones, dtype=torch.float32, device=device) # zeros out terminated\n"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "cart = DQN(N, env)\n",
    "\n",
    "episodes = 600\n",
    "epsilon = .8\n",
    "discount = .99\n",
    "action = cart.action\n",
    "total_rewards = cart.train(episodes, epsilon, discount, action, False)\n",
    "# cart.save(\"drpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\")\n",
    "reward_print(total_rewards, episodes, \"CartPole\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = cart.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"CartPole\")\n",
    "          # policy_indices = self.policy_net(next_state_batch[done_mask.bool()]).max(1).indices\n",
    "          # next_state_values[done_mask.bool()] = self.target_net(next_state_batch[done_mask.bool()]).gather(1, policy_indices.unsqueeze(1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'layer1.weight': tensor([[-0.1163,  0.0632,  0.3759, -0.1103],\n",
      "        [-0.0551,  0.0651,  0.0883, -0.3082],\n",
      "        [ 0.1176,  0.1558, -0.3363, -0.2061],\n",
      "        [ 0.3504,  0.3565,  0.0503,  0.2385],\n",
      "        [-0.0135, -0.0622, -0.4049,  0.2239],\n",
      "        [ 0.2374, -0.0444, -0.2621, -0.1243],\n",
      "        [ 0.0484, -0.2172,  0.1059,  0.0079],\n",
      "        [-0.3850, -0.2161, -1.0756, -0.1640],\n",
      "        [ 0.0104, -0.1283, -0.1561, -0.1733],\n",
      "        [-0.5888, -0.1871,  0.9653, -0.0857],\n",
      "        [ 0.2225, -0.2026, -0.3287, -0.1285],\n",
      "        [ 0.0977,  0.1676,  0.9415,  0.2195],\n",
      "        [-0.0490,  0.0683, -0.0306, -0.0864],\n",
      "        [-0.5207, -0.1740,  0.0105, -0.0547],\n",
      "        [ 0.3841, -0.0532, -0.9298, -0.2318],\n",
      "        [ 0.1776,  0.2235, -0.9380,  0.0086],\n",
      "        [ 0.1416, -0.2902,  0.2632, -0.0367],\n",
      "        [-0.1603,  0.1229, -0.1470,  0.0754],\n",
      "        [ 0.0154,  0.1364,  0.3883,  0.2083],\n",
      "        [-0.1947,  0.0939, -1.1772, -0.1307],\n",
      "        [-0.0941, -0.3262, -0.8575, -0.2281],\n",
      "        [ 0.4641, -0.0916, -1.5336, -0.2110],\n",
      "        [-0.1459,  0.1098,  1.2355,  0.2474],\n",
      "        [ 0.0617,  0.0059, -0.2110,  0.0683],\n",
      "        [-0.0187, -0.0506, -0.2904, -0.1674],\n",
      "        [ 0.2029, -0.0074,  0.6905,  0.1676],\n",
      "        [-0.3099, -0.1504, -1.1104, -0.1522],\n",
      "        [ 0.0865,  0.1452,  0.9729,  0.1718],\n",
      "        [ 0.5134,  0.0407, -0.8709, -0.2344],\n",
      "        [-0.2506, -0.3494, -0.9227, -0.3001],\n",
      "        [-0.3142,  0.1189, -0.0695,  0.1053],\n",
      "        [-0.0548, -0.0571, -0.2277, -0.1123]], device='cuda:0'), 'layer1.bias': tensor([ 0.7224,  0.8529, -0.5665,  0.7356,  0.6455,  0.8560,  0.7806, -0.7947,\n",
      "         0.7624, -0.6992,  0.7327, -0.5667,  0.8823, -0.3619, -0.4615, -0.4825,\n",
      "         0.7121,  0.9352,  0.7158, -0.3652,  0.5598, -0.3832, -0.3662, -0.2075,\n",
      "         0.7466, -0.3469, -0.4183, -0.0921, -0.0752, -0.3717,  0.6752,  0.8375],\n",
      "       device='cuda:0'), 'layer2.weight': tensor([[ 7.3959e-02,  9.7231e-02,  1.1488e-01,  ...,  3.9941e-01,\n",
      "          1.2527e-01,  1.0326e-01],\n",
      "        [ 4.6328e-01,  3.9862e-01,  1.2916e-02,  ..., -5.3885e-01,\n",
      "          3.3747e-01,  4.8046e-01],\n",
      "        [ 5.7858e-01,  6.3235e-01,  8.7275e-01,  ..., -1.7936e+00,\n",
      "          6.5942e-01,  5.3120e-01],\n",
      "        ...,\n",
      "        [ 4.8209e-01,  2.7743e-01, -3.3394e-02,  ..., -5.8277e-01,\n",
      "          5.1938e-01,  3.6108e-01],\n",
      "        [ 1.0724e-03, -1.3984e-01, -2.6552e-02,  ..., -5.0499e-03,\n",
      "         -1.2650e-01, -4.4560e-01],\n",
      "        [-9.8977e-02, -4.6865e-02,  3.6905e-02,  ..., -9.7973e-02,\n",
      "         -9.8321e-02, -8.1525e-02]], device='cuda:0'), 'layer2.bias': tensor([-0.0331,  0.4661,  0.6674,  0.0234,  0.4575,  0.5470, -0.0357, -0.1278,\n",
      "         0.0624,  0.4298,  0.0237,  0.0715,  0.4641, -0.0012, -0.3887,  0.4666,\n",
      "        -0.2261,  0.4060,  0.5939, -0.0653,  0.5181,  0.4927,  0.5598,  0.5798,\n",
      "         0.5448, -0.0910,  0.4900,  0.4944,  0.5988,  0.3952, -0.0948, -0.0635],\n",
      "       device='cuda:0'), 'layer3.weight': tensor([[-2.0868e-01,  6.8880e-01,  2.5224e-01, -7.1194e-02,  7.3400e-01,\n",
      "          9.0816e-01,  4.6601e-03, -5.7658e-04,  9.4253e-02,  5.9186e-01,\n",
      "         -2.2040e-02, -7.6540e-02,  6.8470e-01,  2.3442e-01, -2.5008e+00,\n",
      "          6.8669e-01, -8.6828e-01,  6.5879e-01,  5.9295e-01, -5.6198e-02,\n",
      "          5.6334e-01,  6.4875e-01,  7.0746e-01,  6.0738e-01,  5.2314e-01,\n",
      "          8.0630e-04,  6.6725e-01,  5.9268e-01,  5.8965e-01,  8.4364e-01,\n",
      "         -1.3361e+00, -3.2177e-02],\n",
      "        [-4.8383e-01,  6.7108e-01,  9.9170e-01,  3.3822e-02,  6.5363e-01,\n",
      "          4.8043e-01,  1.7802e-02, -4.5851e-02,  8.3012e-02,  6.6887e-01,\n",
      "          7.0866e-03, -3.9710e-02,  4.8596e-01, -3.2561e-01, -2.0349e+00,\n",
      "          7.1734e-01, -1.2929e+00,  6.0703e-01,  6.4870e-01,  9.7146e-02,\n",
      "          6.0081e-01,  6.7513e-01,  6.8343e-01,  7.0366e-01,  7.0961e-01,\n",
      "          6.4064e-02,  5.4029e-01,  6.5330e-01,  6.2919e-01,  2.3811e-01,\n",
      "         -8.5760e-01,  1.5249e-02]], device='cuda:0'), 'layer3.bias': tensor([0.4059, 0.2943], device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "print(cart.policy_net.state_dict())\n",
    "# cart.save(\"drpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
