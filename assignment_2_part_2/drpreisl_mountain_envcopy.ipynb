{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:51.775833Z",
     "start_time": "2024-10-31T19:01:51.771501Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:55.134415Z",
     "start_time": "2024-10-31T19:01:55.130999Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:57.780803Z",
     "start_time": "2024-10-31T19:01:57.776274Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:14:53.327192Z",
     "start_time": "2024-10-31T19:14:53.315948Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, env):\n",
    "    # initialize environment\n",
    "    self.env = env\n",
    "    state, info = self.env.reset()\n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = ReplayMemory(N)\n",
    "    self.pointer = 0\n",
    "    self.policy_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    self.target_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    self.optimizer = optim.AdamW(self.policy_net.parameters(),amsgrad=True ) #auto learning rate\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    # TAU = .0004\n",
    "    for i in range(episodes):\n",
    "      if i % 5 == 0:\n",
    "         self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      seq  = [None , None]\n",
    "      state, info = self.env.reset()\n",
    "      seq[0] = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "      done = False\n",
    "      rewards = 0\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      while not done:\n",
    "        # Select action\n",
    "        action_type = action_function(seq[0], eps)\n",
    "        observation, reward, terminated, truncated, _ = self.env.step(action_type.item())\n",
    "        state = torch.tensor(observation, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        # Set sequence\n",
    "        seq[1] = state\n",
    "        if terminated:\n",
    "           seq[1] = None\n",
    "        rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        # store transition in replay buffer\n",
    "        self.replay.push(seq[0], action_type,  seq[1], reward)\n",
    "        seq[0] = state\n",
    "        \n",
    "        self.optimize_function(discount)\n",
    "\n",
    "        # target_net_state_dict = self.target_net.state_dict()\n",
    "        # policy_net_state_dict = self.policy_net.state_dict()\n",
    "        # for key in policy_net_state_dict:\n",
    "        #     target_net_state_dict[key] = policy_net_state_dict[key]* TAU + target_net_state_dict[key]*(1- TAU)\n",
    "        \n",
    "\n",
    "        # self.target_net.load_state_dict(target_net_state_dict)\n",
    "        done = truncated or terminated\n",
    "      # Decay epsilon after every episode\n",
    "      epsilon *= epsilon\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "      \n",
    "  def mountain_car_action(self, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_type = self.env.action_space.sample()\n",
    "    else:\n",
    "        # select max(Q)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).max(1).indices.view(1, 1) \n",
    "    return torch.tensor([[action_type]], device=device, dtype=torch.long)\n",
    "  \n",
    "  def optimize_function(self, discount):\n",
    "    BATCH_SIZE = 256\n",
    "    if len(self.replay) < BATCH_SIZE:\n",
    "        return\n",
    "    else:\n",
    "        transitions = self.replay.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        expected_state_action_values = (next_state_values * discount) + reward_batch\n",
    "             \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net.state_dict(), file,protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:19:10.107111Z",
     "start_time": "2024-10-31T19:19:10.101012Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prints the reward per epsisode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - int(min(reward_per_episode)) * (.15)\n",
    "    maxs = int(max(reward_per_episode)) + int(max(reward_per_episode)) * (.3) \n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:29:55.191731Z",
     "start_time": "2024-10-31T19:29:40.566273Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N = 5000\n",
    "# env = gym.make(\"MountainCar-v0\")\n",
    "# env.reset()\n",
    "# MtnCar = DQN(N, env)\n",
    "\n",
    "# episodes = 20\n",
    "# epsilon = .8\n",
    "# discount = .4\n",
    "# action = MtnCar.mountain_car_action\n",
    "# total_rewards = MtnCar.train(episodes, epsilon, discount, action, False)\n",
    "# # MtnCar.save(\"drpreisl_angustsa_assignment2_part2_dqn_mountaincar.pickle\")\n",
    "# reward_print(total_rewards, episodes, \"MountainCar\")\n",
    "# ep_decay(epsilon, episodes)\n",
    "# total_rewards = MtnCar.train(6, epsilon, discount, action, True)\n",
    "# reward_print(total_rewards, 5, \"MountainCar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T21:41:36.703999Z",
     "start_time": "2024-10-31T21:41:36.241784Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 5000\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "cart = DQN(N, env)\n",
    "\n",
    "episodes = 600\n",
    "epsilon = .8\n",
    "discount = .99\n",
    "action = cart.mountain_car_action\n",
    "total_rewards = cart.train(episodes, epsilon, discount, action, False)\n",
    "# cart.save(\"drpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\")\n",
    "reward_print(total_rewards, episodes, \"CartPole\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = cart.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'layer1.weight': tensor([[-2.4114e-01, -4.0490e-01, -1.2699e+00, -3.0654e-01],\n",
      "        [-2.9313e-01,  2.8005e-03,  1.2193e-01,  4.4894e-02],\n",
      "        [-2.8639e-01,  2.9199e-01,  7.0606e-01,  6.6252e-02],\n",
      "        [ 3.7325e-01, -1.5107e-01, -5.9602e-01, -1.7507e-01],\n",
      "        [ 2.5051e-01,  5.5204e-01,  8.0034e-01,  3.3672e-01],\n",
      "        [ 1.0715e-02, -8.7362e-02,  1.3136e-01,  2.6932e-01],\n",
      "        [ 2.7521e-02, -8.5690e-02,  4.3459e-02, -1.9604e-01],\n",
      "        [-9.4617e-01, -3.4654e-01, -3.5285e-02, -1.8904e-01],\n",
      "        [-5.1114e-02,  1.1432e-01, -6.2896e-02,  5.6709e-02],\n",
      "        [-1.8502e-01,  5.3209e-02,  1.1649e-03, -8.5723e-02],\n",
      "        [ 6.6413e-02,  1.2468e-01,  1.7362e+00,  3.3517e-01],\n",
      "        [ 4.2969e-01, -1.3064e-01, -6.2564e-01, -1.2409e-01],\n",
      "        [ 2.2818e-01,  3.3037e-01,  4.5649e-01,  8.2085e-02],\n",
      "        [-1.2056e-01,  3.6145e-01,  1.3152e+00, -2.8129e-02],\n",
      "        [-3.0655e-01,  5.3420e-02,  4.6215e-01,  1.7900e-01],\n",
      "        [ 3.6173e-02, -8.4208e-02,  2.1184e-01,  1.0478e-01],\n",
      "        [-6.1978e-01,  2.3795e-01,  3.7482e-01,  3.3584e-01],\n",
      "        [ 8.7243e-01,  3.5088e-01,  1.0520e+00,  2.5421e-01],\n",
      "        [ 1.9806e-01, -1.3070e-01, -2.0995e-01, -2.0763e-01],\n",
      "        [ 4.1463e-01,  1.1955e-01, -4.8485e-01, -2.2483e-01],\n",
      "        [-4.8765e-01, -1.9196e-01,  8.5613e-01,  8.4070e-02],\n",
      "        [ 7.7921e-02,  2.4120e-02,  2.1471e-01,  1.2625e-01],\n",
      "        [ 1.1939e-01,  1.5118e-01,  6.9908e-01,  1.7152e-01],\n",
      "        [ 1.8713e-01,  7.6427e-02, -7.6141e-01,  2.6003e-02],\n",
      "        [ 1.1123e-01, -6.3749e-02, -1.0217e+00, -5.9780e-01],\n",
      "        [ 3.6022e-01, -1.1539e-01, -5.5447e-01, -1.9691e-01],\n",
      "        [ 1.7617e-01,  1.5774e-02,  2.4872e-03,  4.5667e-02],\n",
      "        [ 6.7727e-02, -8.9262e-02, -1.2197e+00, -4.3631e-01],\n",
      "        [ 3.7062e-01, -1.1796e-01, -8.6906e-01,  7.9254e-02],\n",
      "        [-1.6105e-01, -1.9218e-01, -1.3587e+00, -4.5689e-01],\n",
      "        [ 1.5724e-01, -1.6116e-01, -7.5216e-02, -2.0096e-02],\n",
      "        [ 2.5696e-01,  1.0662e-01, -5.7607e-01,  6.9555e-02]], device='cuda:0'), 'layer1.bias': tensor([ 0.0154,  0.8296,  0.9451,  0.7961,  0.2809,  0.8238,  0.8246, -0.0490,\n",
      "        -0.2110,  0.8841, -0.4744,  0.7757,  0.4616,  0.7189,  0.8863,  0.8903,\n",
      "         0.5025, -0.4677,  0.8275, -0.5258, -0.4384,  0.7264,  0.7156,  0.7361,\n",
      "        -0.4683,  0.8909,  0.8741, -0.3575,  0.5626, -0.2032, -0.2344,  0.7299],\n",
      "       device='cuda:0'), 'layer2.weight': tensor([[ 0.6114, -0.0022,  0.1220,  ...,  1.0228,  0.0154, -1.2666],\n",
      "        [ 0.5216, -0.4518, -0.0875,  ...,  0.4399, -0.0713, -0.0147],\n",
      "        [ 0.0130,  0.0255, -0.0623,  ...,  0.0066,  0.0157, -0.0918],\n",
      "        ...,\n",
      "        [-0.0800, -0.0560, -0.0920,  ...,  0.0455, -0.0694, -0.0399],\n",
      "        [ 1.1681,  0.0331, -0.1188,  ...,  0.5656,  0.0904, -0.2202],\n",
      "        [-0.8589,  0.6026,  0.3091,  ..., -0.1274,  0.0110,  0.6963]],\n",
      "       device='cuda:0'), 'layer2.bias': tensor([-0.1894, -0.3352, -0.0892,  0.4150, -0.0537,  0.5246,  0.4821, -0.2951,\n",
      "        -0.1249,  0.0342, -0.0855,  0.3578,  0.3261, -0.1696, -0.0763,  0.4088,\n",
      "        -0.1399, -0.0392,  0.5025,  0.3388, -0.0784,  0.4007,  0.4451,  0.0107,\n",
      "         0.5608,  0.6800,  0.4041,  0.4710, -0.0934, -0.0007, -0.1500,  0.4852],\n",
      "       device='cuda:0'), 'layer3.weight': tensor([[-1.4431e+00, -4.4620e-02, -3.0210e-04,  6.3800e-01,  8.0009e-02,\n",
      "          6.1374e-01,  5.5964e-01,  1.6071e-01, -1.2866e+00, -4.0218e-02,\n",
      "         -3.9805e-02,  4.9871e-01,  5.4862e-01, -8.0894e-01, -4.8818e-01,\n",
      "          6.3361e-01, -1.3027e+00,  7.2581e-03,  6.1870e-01,  4.8623e-01,\n",
      "         -5.6542e-02,  4.7791e-01,  5.6975e-01, -3.6004e-01,  7.5103e-01,\n",
      "          7.3558e-01,  5.5962e-01,  5.1100e-01,  7.5275e-04,  6.5717e-02,\n",
      "         -1.8797e+00,  5.3207e-01],\n",
      "        [-1.2496e+00, -1.2642e+00, -7.4644e-02,  5.3656e-01,  1.7387e-02,\n",
      "          5.2120e-01,  5.2133e-01, -9.0446e-01, -1.1488e+00, -6.5508e-02,\n",
      "         -8.1501e-04,  6.2491e-01,  6.2323e-01, -8.7508e-01, -1.3903e+00,\n",
      "          5.2282e-01, -1.4226e+00, -4.6213e-02,  6.4584e-01,  6.6738e-01,\n",
      "          6.1263e-02,  6.1289e-01,  5.7945e-01,  1.2189e-01,  4.9482e-01,\n",
      "          8.0052e-01,  6.5046e-01,  5.3468e-01, -4.6389e-02,  4.4232e-02,\n",
      "         -1.8548e+00,  4.8713e-01]], device='cuda:0'), 'layer3.bias': tensor([0.4439, 0.2550], device='cuda:0')})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary +: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(cart\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 96\u001b[0m, in \u001b[0;36mDQN.save\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 96\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     97\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mstate_dict(), file,protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCO)\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary +: 'str'"
     ]
    }
   ],
   "source": [
    "print(cart.policy_net.state_dict())\n",
    "cart.save(\"drpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
