{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T05:52:24.821987Z",
     "start_time": "2024-10-31T05:52:24.756774Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T05:52:24.834502Z",
     "start_time": "2024-10-31T05:52:24.823985Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T05:52:24.843919Z",
     "start_time": "2024-10-31T05:52:24.835501Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T06:00:53.043989Z",
     "start_time": "2024-10-31T06:00:53.034511Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, C, env):\n",
    "    # initialize environment\n",
    "    self.env = env\n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = ReplayMemory(N)\n",
    "    self.pointer = 0\n",
    "    self.policy_net = Net(3, 2)\n",
    "    self.target_net = Net(3, 2)\n",
    "    # initialize action-value function Q with random weights 0\n",
    "    self.action_value = None\n",
    "\n",
    "    self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "\n",
    "    self.C = C\n",
    "    pass\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function):\n",
    "    total_reward = [0] * episodes  \n",
    "    for i in range(episodes):\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      seq  = [None , None]\n",
    "      seq[0] = torch.tensor([0,0], dtype=torch.float32).unsqueeze(0)\n",
    "      terminated = False\n",
    "      t = rewards = 0\n",
    "      state, info = self.env.reset()\n",
    "      while not terminated:\n",
    "        # Select action\n",
    "        action = action_function(state, epsilon)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "        # Set sequence\n",
    "        seq[1] = state       \n",
    "        rewards += reward\n",
    "        # store transition in replay buffer\n",
    "        self.replay.push(seq[0], action,  seq[1], reward)\n",
    "        seq[0] = state\n",
    "        # Every C steps reset Q' = Q\n",
    "        t += 1\n",
    "        if t % self.C == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            self.replay_function(discount)\n",
    "        \n",
    "      # Decay epsilon after every episode\n",
    "      epsilon *= epsilon\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "      \n",
    "  def mountain_car_action(self, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_type = np.random.randint(3)\n",
    "    else:\n",
    "        # select max(Q)\n",
    "        with torch.no_grad():\n",
    "            action_type = self.policy_net(state).max(1).indices.view(1, 1).item()    \n",
    "    return action_type\n",
    "  \n",
    "  def replay_function(self, discount):\n",
    "    if len(self.replay) < 128:\n",
    "        return\n",
    "    else:\n",
    "            #print(\"sample\")\n",
    "            sample = self.replay.sample(128)\n",
    "            q_values_list = []\n",
    "            target_list = []\n",
    "            for state, action, next_state, reward in sample:\n",
    "                if next_state == None:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        #predict target Q-values\n",
    "                        q = self.target_net(next_state).max(1)[0]\n",
    "                        next_q = q.item() #predict next state with target network \n",
    "                        target = (reward + discount * np.amax(next_q))\n",
    "                q_values = self.policy_net(state)[0][action].item()#predict with policy network\n",
    "                q_values_list.append(q_values)\n",
    "                target_list.append(target)\n",
    "            # Set loss function \n",
    "            loss = nn.MSELoss()\n",
    "            # Set action lists require grad\n",
    "            state_action_list = torch.FloatTensor(q_values_list)\n",
    "            state_action_list.requires_grad = True\n",
    "            target_list = torch.FloatTensor(target_list)\n",
    "            target_list.requires_grad = True\n",
    "            # back prop\n",
    "            self.optimizer.zero_grad()\n",
    "            output = loss(state_action_list, target_list.unsqueeze(1))\n",
    "            output.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(\"pickles/\" + filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T05:52:24.926033Z",
     "start_time": "2024-10-31T05:52:24.915969Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints the reward per epsisode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) + int(min(reward_per_episode)) * (.2)\n",
    "    maxs = int(max(reward_per_episode)) - int(max(reward_per_episode)) * (.2) \n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 3x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m discount \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.4\u001B[39m\n\u001B[0;32m     10\u001B[0m action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mmountain_car_action\n\u001B[1;32m---> 11\u001B[0m total_rewards \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m agent\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmountain_car\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     13\u001B[0m reward_print(total_rewards, episodes, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMountainCar\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[18], line 32\u001B[0m, in \u001B[0;36mDQN.train\u001B[1;34m(self, episodes, epsilon, discount, action_function)\u001B[0m\n\u001B[0;32m     29\u001B[0m state, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m terminated:\n\u001B[0;32m     31\u001B[0m   \u001B[38;5;66;03m# Select action\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m   action \u001B[38;5;241m=\u001B[39m \u001B[43maction_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m   observation, reward, terminated, truncated, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     34\u001B[0m   state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(observation, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "Cell \u001B[1;32mIn[18], line 59\u001B[0m, in \u001B[0;36mDQN.mountain_car_action\u001B[1;34m(self, state, epsilon)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;66;03m# select max(Q)\u001B[39;00m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 59\u001B[0m         action_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mindices\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mitem()    \n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m action_type\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[5], line 10\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 10\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     11\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x))\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 3x128)"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "C = 3\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "agent = DQN(N, C, env)\n",
    "\n",
    "episodes = 2\n",
    "epsilon = .8\n",
    "discount = .4\n",
    "action = agent.mountain_car_action\n",
    "total_rewards = agent.train(episodes, epsilon, discount, action)\n",
    "agent.save(\"mountain_car\")\n",
    "reward_print(total_rewards, episodes, \"MountainCar\")\n",
    "ep_decay(epsilon, episodes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T06:00:57.905372Z",
     "start_time": "2024-10-31T06:00:57.408664Z"
    }
   },
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
