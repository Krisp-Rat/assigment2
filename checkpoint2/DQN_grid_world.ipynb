{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "\n",
    "      self.observation_space = spaces.Discrete(36)\n",
    "      self.action_space = spaces.Discrete(6)\n",
    "      self.max_timesteps = 150\n",
    "\n",
    "      self.timestep = 0\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.goal_pos = [5, 5]\n",
    "      self.package_pos = [3, 1]\n",
    "      self.state = np.zeros((6,6))\n",
    "      #Shelves\n",
    "      self.wall_1 = [3,2]\n",
    "      self.wall_2 = [4,2]\n",
    "      self.wall_3 = [5,2]\n",
    "      self.wall_4 = [1,4]\n",
    "      self.wall_5 = [1,5]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "\n",
    "      self.package = 0\n",
    "      self.pickup = 0\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.timestep = 0\n",
    "      observation = self.state.flatten()\n",
    "      self.package =0\n",
    "      info = {}\n",
    "\n",
    "      return observation, info\n",
    "\n",
    "    # Step function: Contains the implementation for what happens when an\n",
    "    # agent takes a step in the environment.\n",
    "    def step(self, action):\n",
    "      prev = self.agent_pos.copy()\n",
    "      if action == 0: #down\n",
    "        self.agent_pos[0] += 1\n",
    "      if action == 1: #up\n",
    "        self.agent_pos[0] -= 1\n",
    "      if action == 2: #right\n",
    "        self.agent_pos[1] += 1\n",
    "      if action == 3: #left\n",
    "        self.agent_pos[1] -= 1\n",
    "\n",
    "      reward = -1\n",
    "      if action == 4: # Pick up\n",
    "        if np.array_equal(self.agent_pos, self.package_pos) and self.package == 0: #Picked up, in right location\n",
    "          self.package = 1\n",
    "          reward = 40\n",
    "          #print(\"picked up\", self.timestep)\n",
    "          #self.pickup += 1\n",
    "        elif self.package: #Picked up while holding a package\n",
    "          reward = -100\n",
    "        else: # Picked up in wrong location\n",
    "          reward = -10\n",
    "\n",
    "      if action == 5:\n",
    "        #Drop off\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and self.package == 1: #Dropped off in right location\n",
    "          reward = 100\n",
    "          #print(\"dropped off\", self.timestep)\n",
    "          self.package = 0\n",
    "          self.timestep = self.max_timesteps\n",
    "        elif self.package == 0: #Dropped off without holding a package\n",
    "          reward = -100\n",
    "        else: #dropped off in wrong location\n",
    "          reward -10\n",
    "\n",
    "      if np.array_equal(self.agent_pos, self.wall_1) or np.array_equal(self.agent_pos, self.wall_2) or np.array_equal(self.agent_pos, self.wall_3) or np.array_equal(self.agent_pos, self.wall_4) or np.array_equal(self.agent_pos, self.wall_5):\n",
    "        reward = -20\n",
    "        self.agent_pos = prev\n",
    "        #print(\"wall\")\n",
    "\n",
    "      # Comment this to demonstrate the truncation condition.\n",
    "      if self.agent_pos[0] > 5 or self.agent_pos[0] < 0 or self.agent_pos[1] > 5 or self.agent_pos[1] < 0:\n",
    "        reward = -25\n",
    "        #print(\"bounding: \", self.agent_pos)\n",
    "      self.agent_pos = np.clip(self.agent_pos, 0, 5)\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      observation = self.state.flatten()\n",
    "\n",
    "\n",
    "      self.timestep += 1\n",
    "\n",
    "      # Condition to check for termination (episode is over)\n",
    "      terminated = True if self.timestep >= self.max_timesteps else False\n",
    "\n",
    "      # Condition to check if agent is traversing to a cell beyond the permitted cells\n",
    "      # This helps the agent to learn how to behave in a safe and predictable manner\n",
    "      truncated = True if np.all((np.asarray(self.agent_pos) >=0 ) & (np.asarray(self.agent_pos) <= 6)) else False\n",
    "      #print(self.agent_pos)\n",
    "\n",
    "      return observation, reward, terminated, self.package\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "      plt.title('Grid Environment')\n",
    "      plt.imshow(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# modify this to fit current environment \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def batch(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs , 64)  \n",
    "        self.fc2 = nn.Linear(64, actions)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        t = torch.tensor([input])\n",
    "        input = F.one_hot(t,36)\n",
    "        input = input.float()\n",
    "        output = F.relu(self.fc1(input))\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "class agent():\n",
    "    def __init__(self,obs,actions):\n",
    "        self.actions = actions\n",
    "        self.batch = 128\n",
    "        self.discount_factor = 0.99\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.9954\n",
    "        # self.tau = 0.005\n",
    "        self.learning_rate = 0.1\n",
    "        self.memory = Memory(5000) # replay memory \n",
    "        # self.policy_net = DQN(obs, actions).to(device) # action value function\n",
    "        # self.target_net = DQN(obs, actions).to(device) # target action value function \n",
    "        self.policy_net = DQN(obs, actions)\n",
    "        self.target_net = DQN(obs, actions)\n",
    "        self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "        self.Q_table = self.print_table() # not working rn\n",
    "        \n",
    "    \n",
    "    def select_action(self,state):\n",
    "        p = random.random()\n",
    "        if p < self.eps:\n",
    "            #random action\n",
    "            return torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n",
    "        else:\n",
    "            #best action\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    def update(self,timestep,episode,terminated):\n",
    "        #update weight every 5 steps\n",
    "        t = timestep +1\n",
    "        if t%5 == 0:\n",
    "            #sample and train policy_net\n",
    "            self.replay()\n",
    "        #update target_net every 5 eps, If solved, set as target_net\n",
    "        ep = episode+1\n",
    "        if ep%5 == 0 or terminated == True:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def replay(self):\n",
    "        if len(agent.memory) < self.batch:\n",
    "            return\n",
    "        else:\n",
    "            sample = self.memory.batch(self.batch)\n",
    "            batch = Transition(*zip(*sample))\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "            print(batch.reward)\n",
    "            print(batch.state)\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "            state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "            next_state_values = torch.zeros(self.batch)\n",
    "            with torch.no_grad():\n",
    "                next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "                expected_state_action_values = (next_state_values * self.learning_rate) + reward_batch\n",
    "                criterion =nn.MSELoss\n",
    "                print(state_action_values)\n",
    "                print(expected_state_action_values)\n",
    "                loss = criterion(state_action_values,expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return \n",
    "        #replay algorithim here\n",
    "    def print_table(self): # not working rn\n",
    "        \n",
    "        return \n",
    "\n",
    "\n",
    "\n",
    "env = GridEnvironment()\n",
    "state, info = env.reset()\n",
    "\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "agent = agent(obs,actions)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, Qtable, C):\n",
    "    # initialize environment\n",
    "    self.env = GridEnvironment()\n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = np.full(N, (0,0,0,0))\n",
    "    self.pointer = 0\n",
    "\n",
    "    # initialize action-value function Q with random weights 0\n",
    "    self.action_value = None\n",
    "\n",
    "    # initialize target action-value function Q with weights 0' = 0\n",
    "\n",
    "    # Qtable: tuple[position(0, 35); holding the pacakge(0,1); action(0,5)] -> estimated reward\n",
    "    self.Qtable = Qtable\n",
    "\n",
    "    self.C = C\n",
    "    pass\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, steps, epsilon, gamma, discount, action_function):\n",
    "    for i in episodes:\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      seq  = [(0,0,0)]\n",
    "      terminated = False\n",
    "      pos = package = 0\n",
    "      self.env.reset()\n",
    "      for S in steps:\n",
    "        # Select action\n",
    "        action, key = action_function(epsilon, pos, package)\n",
    "        # Set sequence\n",
    "        seq[S+1] = key\n",
    "\n",
    "        # Execute action and observe reward\n",
    "        position, reward, terminated, package = self.env.step(action)\n",
    "        pos = np.where(position == 50)[0][0]\n",
    "\n",
    "        # store transition in replay buffer\n",
    "        transition = (seq[S], action, reward, seq[S+1])\n",
    "        self.replay[self.pointer] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "        # Sample mini batches from the replay buffer\n",
    "        self.batching(transition)\n",
    "        if terminated:\n",
    "          # Set Yj to reward\n",
    "          pass\n",
    "        else:\n",
    "          # Set Yj to Q value aprox\n",
    "          pass\n",
    "\n",
    "\n",
    "        # gamma decay\n",
    "        gamma *= gamma\n",
    "        # Set a gradient descent step\n",
    "\n",
    "        # Every C steps reset Q' = Q\n",
    "\n",
    "      # Decay epsilon after every episode\n",
    "      epsilon *= epsilon\n",
    "  # Determine the action for the warehouse environment\n",
    "  def warehouse_action(self, epsilon, pos, package):\n",
    "      if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(self.env.action_space)\n",
    "        key = (pos, package, action)\n",
    "        self.Qtable.get(key, 0)\n",
    "      else:\n",
    "        # select max(Q)\n",
    "        actions = []\n",
    "        for i in range(self.env.action_space):\n",
    "            act = self.Qtable.get((pos, package, i), 0)\n",
    "            actions.append(act)\n",
    "        expected = max(actions)\n",
    "        action = actions.index(expected)\n",
    "        key = (pos, package, action)\n",
    "\n",
    "      return action, key\n",
    "\n",
    "  # Replay function for batching\n",
    "  def batching(self, transition):\n",
    "    pass\n",
    "\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(\"pickles/\" + filename, 'wb') as file:\n",
    "      pickle.dump(self.Qtable, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
