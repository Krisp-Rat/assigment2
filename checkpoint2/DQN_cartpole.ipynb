{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html \n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 Laptop GPU'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports are always needed\n",
    "import torch\n",
    "# get index of currently selected device\n",
    "torch.cuda.current_device() # returns 0 in my case\n",
    "# get number of GPUs available\n",
    "torch.cuda.device_count() # returns 1 in my case\n",
    "# get the name of the device\n",
    "torch.cuda.get_device_name(0) # good old Tesla K80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# modify this to fit current environment \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def batch(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "class agent():\n",
    "    def __init__(self,obs,actions):\n",
    "        self.actions = actions\n",
    "        self.batch = 128\n",
    "        self.discount_factor = 0.99\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.9954\n",
    "        # self.tau = 0.005\n",
    "        self.learning_rate = 0.1\n",
    "        self.memory = Memory(5000) # replay memory \n",
    "        # self.policy_net = DQN(obs, actions).to(device) # action value function\n",
    "        # self.target_net = DQN(obs, actions).to(device) # target action value function \n",
    "        self.policy_net = DQN(obs, actions)\n",
    "        self.target_net = DQN(obs, actions)\n",
    "        self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "        self.Q_table = self.print_table() # not working rn\n",
    "        \n",
    "    \n",
    "    def select_action(self,state):\n",
    "        p = random.random()\n",
    "        if p < self.eps:\n",
    "            #random action\n",
    "            return torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n",
    "        else:\n",
    "            #best action\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    def update(self,timestep,episode,terminated):\n",
    "        #update weight every 5 steps\n",
    "        t = timestep +1\n",
    "        if t%5 == 0:\n",
    "            #sample and train policy_net\n",
    "            self.replay()\n",
    "        #update target_net every 5 eps, If solved, set as target_net\n",
    "        ep = episode+1\n",
    "        if ep%5 == 0 or terminated == True:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def replay(self):\n",
    "        if len(agent.memory) < self.batch:\n",
    "            return\n",
    "        else:\n",
    "            sample = self.memory.batch(self.batch)\n",
    "            batch = Transition(*zip(*sample))\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "            print(batch.reward)\n",
    "            print(batch.state)\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "            state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "            next_state_values = torch.zeros(self.batch)\n",
    "            with torch.no_grad():\n",
    "                next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "                expected_state_action_values = (next_state_values * self.learning_rate) + reward_batch\n",
    "                criterion =nn.MSELoss\n",
    "                print(state_action_values)\n",
    "                print(expected_state_action_values)\n",
    "                loss = criterion(state_action_values,expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return \n",
    "        #replay algorithim here\n",
    "    def print_table(self): # not working rn\n",
    "        \n",
    "        return \n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state, info = env.reset()\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "agent = agent(obs,actions)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop end at: 13\n",
      "loop end at: 10\n",
      "loop end at: 12\n",
      "loop end at: 14\n",
      "loop end at: 29\n",
      "loop end at: 17\n",
      "loop end at: 13\n",
      "(1.0, 18.0, 7.0, 9.0, 1.0, 10.0, 3.0, 8.0, 16.0, 4.0, 8.0, 7.0, 15.0, 3.0, 29.0, 13.0, 4.0, 13.0, 2.0, 8.0, 1.0, 12.0, 10.0, 3.0, 3.0, 24.0, 8.0, 5.0, 12.0, 12.0, 11.0, 6.0, 8.0, 20.0, 2.0, 12.0, 27.0, 12.0, 5.0, 1.0, 8.0, 9.0, 3.0, 16.0, 26.0, 10.0, 13.0, 1.0, 2.0, 6.0, 5.0, 9.0, 22.0, 12.0, 5.0, 10.0, 14.0, 14.0, 9.0, 15.0, 18.0, 1.0, 11.0, 11.0, 14.0, 6.0, 10.0, 10.0, 5.0, 11.0, 9.0, 7.0, 11.0, 1.0, 14.0, 3.0, 4.0, 2.0, 17.0, 5.0, 7.0, 6.0, 6.0, 1.0, 9.0, 13.0, 6.0, 14.0, 2.0, 2.0, 2.0, 13.0, 6.0, 21.0, 9.0, 28.0, 4.0, 11.0, 4.0, 7.0, 4.0, 14.0, 3.0, 15.0, 15.0, 8.0, 4.0, 11.0, 13.0, 7.0, 4.0, 11.0, 9.0, 7.0, 10.0, 3.0, 30.0, 8.0, 25.0, 17.0, 2.0, 23.0, 7.0, 5.0, 19.0, 5.0, 6.0, 12.0)\n",
      "(tensor([[-0.0435, -0.0072,  0.0360, -0.0374]]), tensor([[ 0.0657,  0.9927, -0.0333, -1.3914]]), tensor([[ 0.0188, -0.3697,  0.0217,  0.5817]]), tensor([[-0.0419, -0.7661,  0.0684,  1.2466]]), tensor([[ 0.0430, -0.0457, -0.0297,  0.0293]]), tensor([[-0.0119, -0.2410,  0.0424,  0.3254]]), tensor([[ 0.0249,  0.0213,  0.0112, -0.0208]]), tensor([[-0.0294, -0.1590,  0.1447,  0.4786]]), tensor([[ 0.0378,  0.6024,  0.0047, -0.8040]]), tensor([[ 0.0329, -0.2171, -0.0356,  0.2540]]), tensor([[ 0.0114, -0.1749,  0.0334,  0.2959]]), tensor([[-0.0222, -0.7556,  0.0455,  1.1702]]), tensor([[-0.1225, -0.7751,  0.2094,  1.4715]]), tensor([[-0.0087,  0.0165,  0.0108,  0.0273]]), tensor([[ 0.1648,  0.0384, -0.1947, -0.4040]]), tensor([[-0.0457,  0.0241,  0.2024,  0.4545]]), tensor([[ 0.0286, -0.2396, -0.0109,  0.2961]]), tensor([[-0.1015, -0.7632,  0.1754,  1.3544]]), tensor([[ 0.0284, -0.1737,  0.0058,  0.2701]]), tensor([[-6.1301e-05, -6.0602e-01,  6.9054e-03,  8.1091e-01]]), tensor([[-0.0132,  0.0170,  0.0159,  0.0172]]), tensor([[-0.1124, -0.6047,  0.1481,  1.1204]]), tensor([[-0.0397, -0.1631,  0.1705,  0.5727]]), tensor([[-0.0414, -0.3800,  0.0269,  0.6142]]), tensor([[ 0.0212, -0.3477,  0.0543,  0.6263]]), tensor([[ 0.1578,  0.2206, -0.1658, -0.4109]]), tensor([[-0.0373, -0.5611,  0.0689,  0.8921]]), tensor([[-0.0037, -0.3648,  0.0167,  0.5709]]), tensor([[-0.0902, -0.5664,  0.1551,  1.0173]]), tensor([[-0.0255, -0.6324,  0.0615,  0.9389]]), tensor([[-0.0167, -0.4367,  0.0489,  0.6312]]), tensor([[ 0.0151, -0.6297,  0.0067,  0.8764]]), tensor([[-0.0305, -0.5703,  0.0496,  0.9388]]), tensor([[ 0.1016,  0.9939, -0.0833, -1.4205]]), tensor([[ 0.0421, -0.2404, -0.0291,  0.3125]]), tensor([[-0.0762, -0.5746,  0.1271,  1.0401]]), tensor([[ 0.1673,  0.0331, -0.1879, -0.2850]]), tensor([[-0.0424, -0.1679,  0.1887,  0.6823]]), tensor([[-0.0605, -0.7711,  0.0574,  1.2199]]), tensor([[ 0.0381, -0.0236, -0.0406, -0.0017]]), tensor([[-0.1030, -0.5792,  0.1260,  1.0035]]), tensor([[-0.0122, -0.4110,  0.0231,  0.5204]]), tensor([[ 0.0373, -0.4351, -0.0228,  0.5959]]), tensor([[-0.0762, -1.0269,  0.1393,  1.6294]]), tensor([[ 0.1628,  0.2253, -0.1776, -0.5169]]), tensor([[-0.0572, -0.5719,  0.0933,  0.9761]]), tensor([[-0.0381, -0.4382,  0.0803,  0.6661]]), tensor([[-0.0380,  0.0108,  0.0202,  0.0161]]), tensor([[-0.0129,  0.2119,  0.0162, -0.2704]]), tensor([[ 0.0222, -0.1743,  0.0160,  0.2840]]), tensor([[-0.0676, -0.3997,  0.0695,  0.5993]]), tensor([[-0.0110, -0.0453,  0.0420,  0.0198]]), tensor([[ 0.1374,  0.6064, -0.1348, -0.8993]]), tensor([[-0.0330, -0.6074,  0.0490,  0.8426]]), tensor([[ 0.0033, -0.7395,  0.0855,  1.2484]]), tensor([[-0.0204, -0.2162,  0.0335,  0.2351]]), tensor([[-0.1031, -0.9678,  0.1753,  1.7049]]), tensor([[-0.0469, -0.6343,  0.0936,  0.9830]]), tensor([[-0.0486, -0.7571,  0.0868,  1.2056]]), tensor([[ 0.0296,  0.4075,  0.0150, -0.5160]]), tensor([[-0.1134, -0.6411,  0.1996,  1.1489]]), tensor([[ 0.0020,  0.0260,  0.0068, -0.0258]]), tensor([[-0.0430,  0.0293,  0.1820,  0.3382]]), tensor([[-0.1042, -0.4081,  0.1323,  0.7891]]), tensor([[-0.1168, -0.5706,  0.2025,  1.1213]]), tensor([[-0.0114, -0.5456,  0.1105,  0.9837]]), tensor([[-0.1301, -0.5827,  0.1727,  1.0893]]), tensor([[-0.1000, -0.2114,  0.1231,  0.4603]]), tensor([[-0.0119, -0.3741,  0.0178,  0.6197]]), tensor([[-0.0686, -0.3782,  0.1128,  0.7141]]), tensor([[-0.0997, -0.0148,  0.1204,  0.1322]]), tensor([[-0.0875, -0.4018,  0.0997,  0.6470]]), tensor([[ 0.0126,  0.4088,  0.0342, -0.5460]]), tensor([[0.0233, 0.0438, 0.0476, 0.0115]]), tensor([[-0.0534, -0.2189,  0.0772,  0.2945]]), tensor([[ 0.0333, -0.0225, -0.0351, -0.0274]]), tensor([[-0.0003, -0.1695,  0.0113,  0.2746]]), tensor([[-0.0377, -0.1846,  0.0205,  0.3151]]), tensor([[-0.0967, -0.8337,  0.1719,  1.3832]]), tensor([[ 0.0285, -0.4117, -0.0306,  0.5353]]), tensor([[-0.0875, -0.7729,  0.1008,  1.2630]]), tensor([[ 0.0203, -0.6063, -0.0198,  0.8182]]), tensor([[-0.0756, -0.5957,  0.0815,  0.9130]]), tensor([[ 0.0279,  0.0215,  0.0063, -0.0246]]), tensor([[-0.0326, -0.3558,  0.1543,  0.8132]]), tensor([[-0.0877, -0.7712,  0.1479,  1.3698]]), tensor([[-0.0760, -0.5768,  0.0818,  0.9458]]), tensor([[-0.1327, -0.6088,  0.1880,  1.2186]]), tensor([[ 0.0377, -0.2181, -0.0406,  0.2779]]), tensor([[-0.0437, -0.2028,  0.0353,  0.2664]]), tensor([[ 0.0242, -0.1519,  0.0479,  0.3189]]), tensor([[0.0250, 0.0178, 0.0185, 0.0572]]), tensor([[-0.0194, -0.1792,  0.0302,  0.3327]]), tensor([[ 0.1214,  0.7999, -0.1117, -1.1550]]), tensor([[0.0079, 0.0198, 0.0393, 0.0140]]), tensor([[ 0.1680, -0.1589, -0.1936, -0.0570]]), tensor([[ 0.0142, -0.5435,  0.0668,  0.9355]]), tensor([[-0.0247, -0.4118,  0.0382,  0.5382]]), tensor([[-0.0557, -0.5941,  0.0520,  0.8752]]), tensor([[-0.0224, -0.3521,  0.1301,  0.7277]]), tensor([[ 0.0253, -0.1740,  0.0108,  0.2755]]), tensor([[ 0.0253,  0.2126,  0.0196, -0.2296]]), tensor([[-0.0477, -0.3984,  0.0406,  0.5700]]), tensor([[-0.0596, -0.8306,  0.1133,  1.3035]]), tensor([[-0.0577, -0.0249,  0.0831,  0.0271]]), tensor([[-0.0062, -0.2399,  0.0360,  0.3009]]), tensor([[-0.0083, -0.1788,  0.0114,  0.3234]]), tensor([[-0.1417, -0.7797,  0.1945,  1.4309]]), tensor([[-0.0451, -0.4130,  0.0658,  0.5658]]), tensor([[ 0.0082, -0.4109, -0.0035,  0.5193]]), tensor([[-0.0490, -0.5755,  0.0391,  0.9152]]), tensor([[-0.0750, -0.7596,  0.1297,  1.2667]]), tensor([[-0.1145, -0.7757,  0.1461,  1.3330]]), tensor([[-0.0230, -0.3747,  0.0369,  0.6347]]), tensor([[-0.0637, -0.5632,  0.1109,  0.9414]]), tensor([[-0.0009,  0.0258,  0.0117, -0.0217]]), tensor([[ 0.1656,  0.2356, -0.2028, -0.7512]]), tensor([[-0.0955, -0.2082,  0.1127,  0.3874]]), tensor([[ 0.1622,  0.0282, -0.1741, -0.1748]]), tensor([[ 0.0498,  0.7974, -0.0114, -1.0952]]), tensor([[ 0.0025, -0.1692,  0.0063,  0.2690]]), tensor([[ 0.1496,  0.4133, -0.1528, -0.6519]]), tensor([[ 0.0025, -0.4346,  0.0243,  0.5859]]), tensor([[ 0.0218,  0.0210,  0.0163, -0.0138]]), tensor([[ 0.0856,  0.7980, -0.0611, -1.1093]]), tensor([[ 0.0238, -0.4346, -0.0050,  0.5853]]), tensor([[-0.0110, -0.5601,  0.0282,  0.8688]]), tensor([[ 0.0207,  0.2132,  0.0233, -0.2428]]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Move to the next state\u001b[39;00m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m---> 29\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepisode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mterminated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#update network weights\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated: \u001b[38;5;66;03m# close loop if it ends\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloop end at:\u001b[39m\u001b[38;5;124m'\u001b[39m, timestep)\n",
      "Cell \u001b[1;32mIn[23], line 67\u001b[0m, in \u001b[0;36magent.update\u001b[1;34m(self, timestep, episode, terminated)\u001b[0m\n\u001b[0;32m     64\u001b[0m t \u001b[38;5;241m=\u001b[39m timestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m#sample and train policy_net\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#update target_net every 5 eps, If solved, set as target_net\u001b[39;00m\n\u001b[0;32m     69\u001b[0m ep \u001b[38;5;241m=\u001b[39m episode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[23], line 88\u001b[0m, in \u001b[0;36magent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mstate)\n\u001b[0;32m     87\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39maction)\n\u001b[1;32m---> 88\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m state_action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net(state_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[0;32m     90\u001b[0m next_state_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got float"
     ]
    }
   ],
   "source": [
    "#create environment to run DQN\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "max_episodes = 600\n",
    "max_timestep = 500\n",
    "state, info = env.reset()\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "# agent = agent(obs,actions)\n",
    "reward_per_episode = []\n",
    "for episode in range(max_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    terminated,truncated = False, False\n",
    "    for timestep in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        # reward +=timestep\n",
    "        if terminated:\n",
    "            next_state = None #used to check if object has fallen during replay\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "        # reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        agent.update(timestep=timestep,episode=episode,terminated=truncated) #update network weights\n",
    "        if terminated or truncated: # close loop if it ends\n",
    "            print('loop end at:', timestep)\n",
    "            reward_per_episode.append(timestep)\n",
    "            break\n",
    "    agent.eps = agent.eps * agent.eps_decay #eps decay\n",
    "\n",
    "print('solved',truncated)\n",
    "plt.figure()\n",
    "fig ,  ax = plt.subplots()\n",
    "plt.xlabel('Episode', fontsize=20)\n",
    "plt.ylabel('Timestep', fontsize=20)\n",
    "plt.title('Cumulative Reward Per Episode', fontsize=24)\n",
    "ax.plot(reward_per_episode,linestyle='solid',label = 'Q learning')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DQN' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DQN' object is not subscriptable"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
