{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 Laptop GPU'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports are always needed\n",
    "import torch\n",
    "# get index of currently selected device\n",
    "torch.cuda.current_device() # returns 0 in my case\n",
    "# get number of GPUs available\n",
    "torch.cuda.device_count() # returns 1 in my case\n",
    "# get the name of the device\n",
    "torch.cuda.get_device_name(0) # good old Tesla K80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# modify this to fit current environment \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def batch(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "class agent():\n",
    "    def __init__(self,obs,actions):\n",
    "        self.actions = actions\n",
    "        self.batch = 128\n",
    "        self.discount_factor = 0.99\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.9954\n",
    "        # self.tau = 0.005\n",
    "        self.learning_rate = 0.1\n",
    "        self.memory = Memory(5000) # replay memory \n",
    "        # self.policy_net = DQN(obs, actions).to(device) # action value function\n",
    "        # self.target_net = DQN(obs, actions).to(device) # target action value function \n",
    "        self.policy_net = DQN(obs, actions)\n",
    "        self.target_net = DQN(obs, actions)\n",
    "        self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "        self.Q_table = self.print_table() # not working rn\n",
    "        \n",
    "    \n",
    "    def select_action(self,state):\n",
    "        p = random.random()\n",
    "        if p < self.eps:\n",
    "            #random action\n",
    "            return torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n",
    "        else:\n",
    "            #best action\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    def update(self,timestep,terminated):\n",
    "        #update weight every 5 steps\n",
    "        t = timestep +1\n",
    "        if t%5 == 0:\n",
    "            #sample and train policy_net\n",
    "            self.replay()\n",
    "        #update target_net every 5 eps, If solved, set as target_net\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(agent.memory) < self.batch:\n",
    "            return\n",
    "        else:\n",
    "            sample = self.memory.batch(self.batch)\n",
    "            for state, action, next_state, reward in sample:\n",
    "                if next_state == None:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    next_q = self.target_net(next_state).max(1)\n",
    "                    print('q',next_q)\n",
    "                    target = (reward + self.learning_rate * np.amax(next_q))\n",
    "                q_values = self.policy_net(state)\n",
    "                q_values[action] = target\n",
    "                self.policy_net.fit(state,q_values)\n",
    "\n",
    "            \n",
    "        #replay algorithim here\n",
    "    def print_table(self): # not working rn\n",
    "        \n",
    "        return \n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state, info = env.reset()\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "agent = agent(obs,actions)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop end at: 22\n",
      "loop end at: 31\n",
      "loop end at: 62\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.return_types.max' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Move to the next state\u001b[39;00m\n\u001b[0;32m     30\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m---> 31\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43mterminated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#update network weights\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated: \u001b[38;5;66;03m# close loop if it ends\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloop end at:\u001b[39m\u001b[38;5;124m'\u001b[39m, timestep)\n",
      "Cell \u001b[1;32mIn[131], line 67\u001b[0m, in \u001b[0;36magent.update\u001b[1;34m(self, timestep, terminated)\u001b[0m\n\u001b[0;32m     64\u001b[0m t \u001b[38;5;241m=\u001b[39m timestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m#sample and train policy_net\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[131], line 79\u001b[0m, in \u001b[0;36magent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     next_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m,next_q)\n\u001b[0;32m     81\u001b[0m     target \u001b[38;5;241m=\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(next_q))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'torch.return_types.max' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "#create environment to run DQN\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "max_episodes = 600\n",
    "max_timestep = 500\n",
    "state, info = env.reset()\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "# agent = agent(obs,actions)\n",
    "reward_per_episode = []\n",
    "for episode in range(max_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    if episode%5 == 0 or truncated == True:\n",
    "        agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "    terminated,truncated = False, False\n",
    "    for timestep in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        # reward +=timestep\n",
    "        if terminated:\n",
    "            next_state = None #used to check if object has fallen during replay\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "        # reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        agent.update(timestep=timestep,terminated=truncated) #update network weights\n",
    "        if terminated or truncated: # close loop if it ends\n",
    "            print('loop end at:', timestep)\n",
    "            reward_per_episode.append(timestep)\n",
    "            break\n",
    "    agent.eps = agent.eps * agent.eps_decay #eps decay\n",
    "\n",
    "print('solved',truncated)\n",
    "plt.figure()\n",
    "fig ,  ax = plt.subplots()\n",
    "plt.xlabel('Episode', fontsize=20)\n",
    "plt.ylabel('Timestep', fontsize=20)\n",
    "plt.title('Cumulative Reward Per Episode', fontsize=24)\n",
    "ax.plot(reward_per_episode,linestyle='solid',label = 'Q learning')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DQN' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DQN' object is not subscriptable"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
