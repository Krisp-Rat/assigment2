{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html \n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports are always needed\n",
    "import torch\n",
    "# get index of currently selected device\n",
    "torch.cuda.current_device() # returns 0 in my case\n",
    "# get number of GPUs available\n",
    "torch.cuda.device_count() # returns 1 in my case\n",
    "# get the name of the device\n",
    "torch.cuda.get_device_name(0) # good old Tesla K80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# modify this to fit current environment \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "class agent():\n",
    "    def __init__(self,obs,actions):\n",
    "        self.actions = actions\n",
    "        self.batch = 128\n",
    "        self.discount_factor = 0.99\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.9954\n",
    "        # self.tau = 0.005\n",
    "        self.learning_rate = 0.1\n",
    "        self.memory = ReplayMemory(5000) # replay memory \n",
    "        # self.policy_net = DQN(obs, actions).to(device) # action value function\n",
    "        # self.target_net = DQN(obs, actions).to(device) # target action value function \n",
    "        self.policy_net = DQN(obs, actions)\n",
    "        self.target_net = DQN(obs, actions)\n",
    "        self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "        self.Q_table = self.print_table() # not working rn\n",
    "        \n",
    "    \n",
    "    def select_action(self,state):\n",
    "        p = random.random()\n",
    "        if p < self.eps:\n",
    "            #random action\n",
    "            return torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n",
    "        else:\n",
    "            #best action\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    def update(self,timestep,episode,terminated):\n",
    "        #update weight every 5 steps\n",
    "        t = timestep +1\n",
    "        if t%5 == 0:\n",
    "            #sample and train policy_net\n",
    "            self.replay()\n",
    "        #update target_net every 5 eps, If solved, set as target_net\n",
    "        ep = episode+1\n",
    "        if ep+1%5 == 0 or terminated == True:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def replay(self):\n",
    "        if len(agent.memory) < self.batch:\n",
    "            return\n",
    "        else:\n",
    "            print(\"sample\")\n",
    "            sample = self.memory.sample(self.batch)\n",
    "            q_values_list = []\n",
    "            target_list = []\n",
    "            for state, action, next_state, reward in sample:\n",
    "                if next_state == None:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        #predict target Q-values\n",
    "                        q = self.target_net(next_state).max(1)[0]\n",
    "                        next_q = q.item() #predict next state with target network \n",
    "                        target = (reward + self.discount_factor * np.amax(next_q))\n",
    "                q_values = self.policy_net(state)[0][action].item()#predict with policy network\n",
    "                q_values_list.append(q_values)\n",
    "                target_list.append(target)\n",
    "            criterion = nn.MSELoss()\n",
    "            state_action_list = torch.FloatTensor(q_values_list)\n",
    "            target_list = torch.FloatTensor(target_list)\n",
    "            print(q_values_list)\n",
    "            loss = criterion(state_action_list,target_list.unsqueeze(1))\n",
    "            print(loss)\n",
    "            #backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            return \n",
    "        #replay algorithim here\n",
    "    def print_table(self): # not working rn\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state, info = env.reset()\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "agent = agent(obs,actions)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replay\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "loop end at: 29\n",
      "replay\n",
      "replay\n",
      "loop end at: 13\n",
      "replay\n",
      "loop end at: 8\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "loop end at: 17\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "loop end at: 25\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "replay\n",
      "loop end at: 21\n",
      "replay\n",
      "replay\n",
      "sample\n",
      "[-0.11418771743774414, -0.14178091287612915, 0.03421987593173981, -0.1150372177362442, 0.025614522397518158, -0.10375641286373138, 0.05161172151565552, 0.015187196433544159, 0.011464055627584457, 0.009003967046737671, -0.11929821968078613, -0.10787113755941391, 0.0904654860496521, 0.030137065798044205, 0.056867845356464386, -0.11253327131271362, 0.1221034973859787, 0.04915963113307953, 0.1395852118730545, -0.1289573609828949, -0.12634959816932678, -0.11527119576931, 0.018021710216999054, -0.12076390534639359, 0.03245539963245392, -0.13092269003391266, 0.02394772693514824, 0.008325837552547455, 0.014824442565441132, -0.12158042192459106, -0.10549916326999664, 0.011981267482042313, -0.09431764483451843, 0.1824592649936676, 0.13060162961483002, 0.021245643496513367, 0.03347235172986984, -0.037772033363580704, 0.02334696799516678, 0.014315661042928696, 0.018245942890644073, 0.13187043368816376, -0.11973831802606583, 0.04993002116680145, -0.08877551555633545, 0.017157964408397675, -0.08385530114173889, 0.05355104058980942, -0.10563153028488159, -0.10620422661304474, -0.11111459136009216, -0.05626799166202545, 0.051084041595458984, -0.15997382998466492, 0.01929735392332077, 0.03476712852716446, -0.11075884848833084, 0.01567217707633972, 0.01166580617427826, 0.048561565577983856, -0.09723116457462311, -0.12553256750106812, -0.055006243288517, -0.10936306416988373, 0.013661365956068039, 0.015630412846803665, 0.04332105815410614, -0.027579277753829956, -0.12196529656648636, 0.010942474007606506, -0.11898072063922882, -0.13113954663276672, 0.03330161049962044, -0.11066970229148865, -0.13207855820655823, 0.18915745615959167, 0.11549469828605652, -0.14335806667804718, 0.023837167769670486, -0.0644119381904602, -0.11622355133295059, 0.13790780305862427, -0.08676500618457794, -0.1411910355091095, 0.007036767899990082, 0.029680714011192322, 0.020946957170963287, 0.013492666184902191, 0.015592798590660095, 0.03972136974334717, 0.013879120349884033, -0.09458240866661072, 0.00580962561070919, 0.013910412788391113, 0.04839373379945755, -0.09797579050064087, -0.06693713366985321, 0.01365884393453598, 0.09868898242712021, -0.10972267389297485, -0.09340924024581909, -0.07548677921295166, 0.016823146492242813, -0.15241111814975739, -0.01705843210220337, -0.05214209109544754, 0.04350505769252777, -0.04430769756436348, 0.09212453663349152, 0.06652624160051346, -0.13879814743995667, -0.06693260371685028, -0.13062512874603271, 0.021365895867347717, 0.13943971693515778, 0.018166474997997284, -0.12924271821975708, -0.1279679387807846, 0.04643460363149643, 0.0312732458114624, -0.037818022072315216, 0.026675082743167877, 0.0945335328578949, -0.09913109987974167, 0.12936322391033173, 0.10840609669685364, 0.03806780278682709, 0.021600782871246338]\n",
      "tensor(184.1318)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corvi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Move to the next state\u001b[39;00m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m---> 29\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepisode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mterminated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#update network weights\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated: \u001b[38;5;66;03m# close loop if it ends\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloop end at:\u001b[39m\u001b[38;5;124m'\u001b[39m, timestep)\n",
      "Cell \u001b[1;32mIn[3], line 67\u001b[0m, in \u001b[0;36magent.update\u001b[1;34m(self, timestep, episode, terminated)\u001b[0m\n\u001b[0;32m     64\u001b[0m t \u001b[38;5;241m=\u001b[39m timestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m#sample and train policy_net\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#update target_net every 5 eps, If solved, set as target_net\u001b[39;00m\n\u001b[0;32m     69\u001b[0m ep \u001b[38;5;241m=\u001b[39m episode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 103\u001b[0m, in \u001b[0;36magent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#backprop\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 103\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "#create environment to run DQN\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "max_episodes = 600\n",
    "max_timestep = 500\n",
    "state, info = env.reset()\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "# agent = agent(obs,actions)\n",
    "reward_per_episode = []\n",
    "for episode in range(max_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    terminated,truncated = False, False\n",
    "    for timestep in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward +=timestep\n",
    "        if terminated:\n",
    "            next_state = None #used to check if object has fallen during replay\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        agent.update(timestep=timestep,episode=episode,terminated=truncated) #update network weights\n",
    "        if terminated or truncated: # close loop if it ends\n",
    "            print('loop end at:', timestep)\n",
    "            reward_per_episode.append(timestep)\n",
    "            break\n",
    "    agent.eps = agent.eps * agent.eps_decay #eps decay\n",
    "\n",
    "print('solved',truncated)\n",
    "plt.figure()\n",
    "fig ,  ax = plt.subplots()\n",
    "plt.xlabel('Episode', fontsize=20)\n",
    "plt.ylabel('Timestep', fontsize=20)\n",
    "plt.title('Cumulative Reward Per Episode', fontsize=24)\n",
    "ax.plot(reward_per_episode,linestyle='solid',label = 'Q learning')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2343, -0.3906, -0.0229,  0.1607, -1.2356,  0.1253,  0.3842, -0.9020,\n",
      "         -0.1528,  1.1994]])\n"
     ]
    }
   ],
   "source": [
    "target = torch.randn(10).view(1,-1)\n",
    "print(target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
