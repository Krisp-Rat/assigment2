{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html \n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T01:17:58.069037Z",
     "start_time": "2024-10-27T01:17:58.065115Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA GeForce GTX 1660 SUPER'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports are always needed\n",
    "import torch\n",
    "# get index of currently selected device\n",
    "torch.cuda.current_device() # returns 0 in my case\n",
    "# get number of GPUs available\n",
    "torch.cuda.device_count() # returns 1 in my case\n",
    "# get the name of the device\n",
    "torch.cuda.get_device_name(0) # good old Tesla K80\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T01:17:59.909183Z",
     "start_time": "2024-10-27T01:17:59.905198Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03632089  0.04400472 -0.04237615  0.02615733]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# modify this to fit current environment \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "class agent():\n",
    "    def __init__(self,obs,actions):\n",
    "        self.actions = actions\n",
    "        self.batch = 128\n",
    "        self.discount_factor = 0.99\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.9954\n",
    "        # self.tau = 0.005\n",
    "        self.learning_rate = 0.1\n",
    "        self.memory = ReplayMemory(5000) # replay memory \n",
    "        # self.policy_net = DQN(obs, actions).to(device) # action value function\n",
    "        # self.target_net = DQN(obs, actions).to(device) # target action value function \n",
    "        self.policy_net = DQN(obs, actions)\n",
    "        self.target_net = DQN(obs, actions)\n",
    "        self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "        self.Q_table = self.print_table() # not working rn\n",
    "        \n",
    "    \n",
    "    def select_action(self,state):\n",
    "        p = random.random()\n",
    "        if p < self.eps:\n",
    "            #random action\n",
    "            return torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n",
    "        else:\n",
    "            #best action\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    def update(self,timestep,episode,terminated):\n",
    "        #update weight every 5 steps\n",
    "        t = timestep +1\n",
    "        if t%5 == 0:\n",
    "            #sample and train policy_net\n",
    "            self.replay()\n",
    "        #update target_net every 5 eps, If solved, set as target_net\n",
    "        ep = episode+1\n",
    "        if ep+1%5 == 0 or terminated == True:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def replay(self):\n",
    "        if len(agent.memory) < self.batch:\n",
    "            return\n",
    "        else:\n",
    "            print(\"sample\")\n",
    "            sample = self.memory.sample(self.batch)\n",
    "            q_values_list = []\n",
    "            target_list = []\n",
    "            for state, action, next_state, reward in sample:\n",
    "                if next_state == None:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        #predict target Q-values\n",
    "                        q = self.target_net(next_state).max(1)[0]\n",
    "                        next_q = q.item() #predict next state with target network \n",
    "                        target = (reward + self.discount_factor * np.amax(next_q))\n",
    "                q_values = self.policy_net(state)[0][action].item()#predict with policy network\n",
    "                q_values_list.append(q_values)\n",
    "                target_list.append(target)\n",
    "            criterion = nn.MSELoss()\n",
    "            state_action_list = torch.FloatTensor(q_values_list)\n",
    "            target_list = torch.FloatTensor(target_list)\n",
    "            print(q_values_list)\n",
    "            loss = criterion(state_action_list,target_list.unsqueeze(1))\n",
    "            print(loss)\n",
    "            #backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            return \n",
    "        #replay algorithim here\n",
    "    def print_table(self): # not working rn\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state, info = env.reset()\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "agent = agent(obs,actions)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T01:47:25.752184Z",
     "start_time": "2024-10-27T01:47:25.739355Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop end at: 30\n",
      "loop end at: 20\n",
      "loop end at: 21\n",
      "loop end at: 31\n",
      "sample\n",
      "[0.17321006953716278, 0.15832164883613586, -0.12193575501441956, 0.17006728053092957, -0.11164073646068573, 0.16719035804271698, -0.12749534845352173, -0.13986122608184814, 0.1621609330177307, -0.09154041856527328, 0.15535110235214233, 0.171651691198349, -0.12102355062961578, -0.07319142669439316, 0.16889330744743347, 0.041560158133506775, -0.10240452736616135, 0.0056761205196380615, -0.10393916070461273, -0.10222607851028442, -0.10769146680831909, 0.16627781093120575, -0.11273528635501862, 0.15874060988426208, -0.0950411781668663, 0.16426166892051697, 0.23476621508598328, 0.16141393780708313, 0.17101672291755676, -0.11397510766983032, -0.10279591381549835, 0.17100752890110016, 0.16136735677719116, -0.09294819831848145, -0.061377912759780884, 0.17409339547157288, 0.16930729150772095, -0.11097672581672668, 0.1695980429649353, -0.08604257553815842, -0.10822220891714096, -0.12567773461341858, 0.16720831394195557, -0.09253843128681183, 0.21454957127571106, 0.048700809478759766, -0.11456862092018127, -0.10810112953186035, 0.03004714846611023, 0.1718904674053192, -0.10654376447200775, -0.05922543257474899, -0.11884866654872894, 0.16667141020298004, -0.10358479619026184, 0.17327404022216797, 0.15961354970932007, 0.16255399584770203, 0.16404390335083008, 0.02792670577764511, 0.16652604937553406, 0.1620793491601944, -0.09844863414764404, -0.11107348650693893, 0.019479289650917053, 0.1627407670021057, 0.1530779004096985, 0.16699212789535522, 0.15338358283042908, 0.17360025644302368, -0.09944948554039001, -0.10461985319852829, -0.11922195553779602, 0.17332834005355835, -0.07914786040782928, -0.05779337137937546, 0.17065584659576416, 0.16405640542507172, 0.17089387774467468, 0.18451398611068726, 0.17311891913414001, 0.16891038417816162, -0.09186668694019318, -0.11429964005947113, -0.13270601630210876, -0.07644656300544739, 0.16647553443908691, -0.023845918476581573, -0.09974287450313568, 0.005535699427127838, -0.07563215494155884, -0.031014438718557358, 0.162742018699646, 0.1578415185213089, -0.06397122144699097, 0.1665019989013672, 0.16645745933055878, 0.17087316513061523, 0.1663721203804016, -0.11439141631126404, -0.08610191941261292, -0.07616262882947922, -0.025901880115270615, 0.17279228568077087, 0.17462235689163208, -0.09298824518918991, 0.16672584414482117, -0.10011529922485352, -0.08840203285217285, 0.1557723879814148, -0.12164042890071869, 0.1665811836719513, 0.16000506281852722, -0.1129373237490654, -0.1266838163137436, -0.10324998199939728, 0.16634252667427063, -0.02506040781736374, -0.09031061083078384, 0.17489665746688843, -0.003903329372406006, -0.12947507202625275, 0.1703709214925766, -0.10776295512914658, -0.11769907176494598, 0.15476751327514648, -0.06438111513853073, -0.09395862370729446]\n",
      "tensor(260.4533)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 29\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Move to the next state\u001B[39;00m\n\u001B[0;32m     28\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[1;32m---> 29\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimestep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimestep\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepisode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepisode\u001B[49m\u001B[43m,\u001B[49m\u001B[43mterminated\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncated\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#update network weights\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated: \u001B[38;5;66;03m# close loop if it ends\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloop end at:\u001B[39m\u001B[38;5;124m'\u001B[39m, timestep)\n",
      "Cell \u001B[1;32mIn[10], line 67\u001B[0m, in \u001B[0;36magent.update\u001B[1;34m(self, timestep, episode, terminated)\u001B[0m\n\u001B[0;32m     64\u001B[0m t \u001B[38;5;241m=\u001B[39m timestep \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m5\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;66;03m#sample and train policy_net\u001B[39;00m\n\u001B[1;32m---> 67\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m#update target_net every 5 eps, If solved, set as target_net\u001B[39;00m\n\u001B[0;32m     69\u001B[0m ep \u001B[38;5;241m=\u001B[39m episode\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[1;32mIn[10], line 102\u001B[0m, in \u001B[0;36magent.replay\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m#backprop\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 102\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\Python\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "#create environment to run DQN\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "max_episodes = 600\n",
    "max_timestep = 500\n",
    "state, info = env.reset()\n",
    "obs = len(state)\n",
    "actions = env.action_space.n\n",
    "# agent = agent(obs,actions)\n",
    "reward_per_episode = []\n",
    "for episode in range(max_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    terminated,truncated = False, False\n",
    "    for timestep in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward +=timestep\n",
    "        if terminated:\n",
    "            next_state = None #used to check if object has fallen during replay\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        agent.update(timestep=timestep,episode=episode,terminated=truncated) #update network weights\n",
    "        if terminated or truncated: # close loop if it ends\n",
    "            print('loop end at:', timestep)\n",
    "            reward_per_episode.append(timestep)\n",
    "            break\n",
    "    agent.eps = agent.eps * agent.eps_decay #eps decay\n",
    "\n",
    "print('solved',truncated)\n",
    "plt.figure()\n",
    "fig ,  ax = plt.subplots()\n",
    "plt.xlabel('Episode', fontsize=20)\n",
    "plt.ylabel('Timestep', fontsize=20)\n",
    "plt.title('Cumulative Reward Per Episode', fontsize=24)\n",
    "ax.plot(reward_per_episode,linestyle='solid',label = 'Q learning')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-27T01:18:05.831901Z",
     "start_time": "2024-10-27T01:18:05.740420Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target = torch.randn(10).view(1,-1)\n",
    "print(target)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
