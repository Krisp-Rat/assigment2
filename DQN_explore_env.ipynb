{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# print(\"Num CPUs Available: \", tf.config.experimental.list_physical_devices('CPU'))\n",
    "\n",
    "# device = torch.device(\n",
    "#     \"cuda\" if torch.cuda.is_available() else\n",
    "#     \"cpu\"\n",
    "# )\n",
    "\n",
    "# print(device)\n",
    "# CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic outline of Environment here \n",
    "\n",
    "# grid = np.zeros((6,6))\n",
    "# print(grid)\n",
    "# Robot_start_pos = [0,0]\n",
    "# package_start_pos = [6,6]\n",
    "# destination_start_pos = [1,6]\n",
    "# grid[tuple(Robot_start_pos)] = 1\n",
    "# plt.imshow(grid)\n",
    "\n",
    "\n",
    "# implement of class Warehouse Robot Here\n",
    "class GridEnvironment(gymnasium.Env):\n",
    "    metadata = {'render.modes': []}\n",
    "    def __init__(self):\n",
    "        self.observation_space = spaces.Discrete(36)\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.max_timesteps = 200\n",
    "\n",
    "        \n",
    "        self.timestep = 0\n",
    "        self.agent_pos = [0,0]\n",
    "        self.package_start_pos = [5,5]\n",
    "        self.dropoff_pos = [5,0]\n",
    "        self.shelf_pos = [[0,1],[1,1],[1,5],[2,3],[4,3],[3,3]]\n",
    "\n",
    "        self.has_package = 0\n",
    "        self.state = np.zeros((6,6))\n",
    "        \n",
    "        self.goal = False\n",
    "        self.state[tuple(self.package_start_pos)] = 0.5\n",
    "        self.state[tuple(self.dropoff_pos)] = 0.25 \n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        for i in self.shelf_pos:\n",
    "            self.state[tuple(i)] = 0.1\n",
    "\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.has_package = 0\n",
    "        self.goal = False\n",
    "        \n",
    "        self.timestep = 0\n",
    "        self.agent_pos = [0,0]\n",
    "        self.package_start_pos = [5,5]\n",
    "        self.dropoff_pos = [5,0]\n",
    "        self.shelf_pos = [[0,1],[1,1],[1,5],[2,3],[4,3],[3,3]]\n",
    "        self.has_package = 0\n",
    "\n",
    "        self.state = np.zeros((6,6))\n",
    "        self.state[tuple(self.package_start_pos)] = 0.5\n",
    "        self.state[tuple(self.dropoff_pos)] = 0.25\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        for i in self.shelf_pos:\n",
    "            self.state[tuple(i)] = 0.1\n",
    "\n",
    "        observation = self.state.flatten()\n",
    "        info = {}\n",
    "        return observation , info\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = -1\n",
    "\n",
    "        # determine action \n",
    "        if action == 0: #down\n",
    "            self.agent_pos[0] +=1\n",
    "            for i in self.shelf_pos:\n",
    "                if np.array_equal(self.agent_pos,i):\n",
    "                    reward = -20\n",
    "                    self.agent_pos[0] -=1\n",
    "        if action == 1: #up\n",
    "            self.agent_pos[0] -=1\n",
    "            for i in self.shelf_pos:\n",
    "                if np.array_equal(self.agent_pos,i):\n",
    "                    reward = -20\n",
    "                    self.agent_pos[0] +=1\n",
    "        if action == 2: #right\n",
    "            self.agent_pos[1] += 1\n",
    "            for i in self.shelf_pos:\n",
    "                if np.array_equal(self.agent_pos,i):\n",
    "                    reward = -20\n",
    "                    self.agent_pos[1] -=1\n",
    "        if action == 3: #left\n",
    "            self.agent_pos[1] -= 1\n",
    "            for i in self.shelf_pos:\n",
    "                if np.array_equal(self.agent_pos,i):\n",
    "                    reward = -20\n",
    "                    self.agent_pos[1] +=1\n",
    "        if action == 4: # pick up\n",
    "            if self.has_package == 0:\n",
    "                if np.array_equal(self.agent_pos,self.package_start_pos):\n",
    "                    self.has_package = 1\n",
    "                    reward = 20\n",
    "                else:\n",
    "                    reward = -20\n",
    "            else:\n",
    "                reward = -20\n",
    "        if action == 5: # drop off:\n",
    "            # sucessful delivery\n",
    "            if self.has_package == 1:\n",
    "                if np.array_equal(self.agent_pos,self.dropoff_pos):\n",
    "                    reward =20\n",
    "                    self.complete =True\n",
    "                    self.goal = True\n",
    "                    self.has_package = 0\n",
    "                else:\n",
    "                    reward = -20\n",
    "            else:\n",
    "                    reward = -20\n",
    "        \n",
    "        # if agent hit wall\n",
    "        check_if_clip = self.agent_pos\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0 ,5) \n",
    "        if np.array_equal(self.agent_pos,check_if_clip) is False:\n",
    "            reward = -20\n",
    "\n",
    "        #package move with agent\n",
    "\n",
    "        if self.has_package == 1:\n",
    "            self.package_start_pos = self.agent_pos\n",
    "            #print(self.package_start_pos)\n",
    "        self.package_start_pos = np.clip(self.package_start_pos, 0, 5)\n",
    "        \n",
    "        self.state = np.zeros((6,6))\n",
    "        # modify to show imgs\n",
    "        self.state[tuple(self.package_start_pos)] = 0.5\n",
    "        self.state[tuple(self.dropoff_pos)] = 0.25\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        for i in self.shelf_pos:\n",
    "            self.state[tuple(i)] = 0.1\n",
    "        observation = self.state.flatten()\n",
    "\n",
    "        \n",
    "        self.timestep += 1\n",
    "        if self.timestep>=self.max_timesteps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = self.goal\n",
    "        #truncated = True if np.all((self.agent_pos >= 0) & (self.agent_pos <=5)) else False\n",
    "        truncated = 0\n",
    "\n",
    "        info = {}\n",
    "        return observation, reward , terminated, truncated, info ,self.goal\n",
    "\n",
    "    def render(self):\n",
    "        plt.title('Grid environment')\n",
    "        plt.imshow(self.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(obs, actions):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(32, input_dim=obs, activation='relu'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(keras.layers.Dense(actions, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae']) #auto learning rate\n",
    "    return model\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, obs, actions):\n",
    "\n",
    "        self.actions = actions\n",
    "        self.discount_factor = 0.99 #gamma\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.9954\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 128\n",
    "        self.size = 5000\n",
    "        self.state_mem = np.zeros((self.size,obs))\n",
    "        self.next_state_mem = np.zeros((self.size,obs))\n",
    "        self.action_mem = np.zeros(self.size, dtype = int )\n",
    "        self.reward_mem = np.zeros(self.size)\n",
    "        self.done_mem = np.zeros(self.size,dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.policy_net = DQN(obs, actions)\n",
    "        self.target_net = DQN(obs, actions) \n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        i = self.pointer % self.size #get index\n",
    "        self.state_mem[i] = state\n",
    "        self.next_state_mem[i] = next_state\n",
    "        self.reward_mem[i] = reward \n",
    "        self.done_mem[i] = done\n",
    "        self.action_mem[i] = action\n",
    "        self.pointer +=1\n",
    "\n",
    "    def sample(self,batch):\n",
    "        mem = min(self.pointer,self.size) # get range to choose mem from\n",
    "        batch = np.random.choice(mem,batch) # choose random indices\n",
    "        states = self.state_mem[batch]\n",
    "        next_states = self.next_state_mem[batch]\n",
    "        actions = self.action_mem[batch]\n",
    "        rewards = self.reward_mem[batch]\n",
    "        done = self.done_mem[batch]\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.actions)  # random action\n",
    "        state = state[np.newaxis,:]\n",
    "        q_values = self.policy_net.predict(state, verbose=0)\n",
    "        return np.argmax(q_values)  # best action\n",
    "\n",
    "    def replay(self):\n",
    "        if self.pointer < self.batch_size:\n",
    "            return\n",
    "        # print(\"learn god damn it\")\n",
    "        state, action, reward, next_state, done = self.sample(self.batch_size)\n",
    "\n",
    "        # estimate cuurent q values\n",
    "        q_eval = self.policy_net.predict(state,verbose = 0)\n",
    "        # set target values\n",
    "        q_next = self.target_net.predict(next_state, verbose = 0)\n",
    "        q_target = q_eval.copy()\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        # print('batch',batch_index)\n",
    "        # print(action)\n",
    "        q_target[batch_index,action] = reward + self.discount_factor *np.max(q_next,axis=1) *done\n",
    "\n",
    "        self.policy_net.fit(state, q_target, verbose=0,)\n",
    "        return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
