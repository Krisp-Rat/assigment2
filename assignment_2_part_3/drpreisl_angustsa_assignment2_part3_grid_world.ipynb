{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:15:53.318908Z",
     "start_time": "2024-10-31T21:15:51.270183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "\n",
    "      self.observation_space = spaces.Discrete(36)\n",
    "      self.action_space = spaces.Discrete(6)\n",
    "      self.max_timesteps = 150\n",
    "\n",
    "      self.timestep = 0\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.goal_pos = [5, 5]\n",
    "      self.package_pos = [3, 1]\n",
    "      self.state = np.zeros((6,6))\n",
    "      #Shelves\n",
    "      self.wall_1 = [3,2]\n",
    "      self.wall_2 = [4,2]\n",
    "      self.wall_3 = [5,2]\n",
    "      self.wall_4 = [1,4]\n",
    "      self.wall_5 = [1,5]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "\n",
    "      self.package = 0\n",
    "      self.pickup = 0\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.agent_pos = [0, 0]\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.timestep = 0\n",
    "      observation = self.state.flatten()\n",
    "      self.package =0\n",
    "      info = {}\n",
    "\n",
    "      return observation, info\n",
    "\n",
    "    # Step function: Contains the implementation for what happens when an\n",
    "    # agent takes a step in the environment.\n",
    "    def step(self, action):\n",
    "      prev = self.agent_pos.copy()\n",
    "      if action == 0: #down\n",
    "        self.agent_pos[0] += 1\n",
    "      if action == 1: #up\n",
    "        self.agent_pos[0] -= 1\n",
    "      if action == 2: #right\n",
    "        self.agent_pos[1] += 1\n",
    "      if action == 3: #left\n",
    "        self.agent_pos[1] -= 1\n",
    "\n",
    "      reward = -1\n",
    "      if action == 4: # Pick up\n",
    "        if np.array_equal(self.agent_pos, self.package_pos) and self.package == 0: #Picked up, in right location\n",
    "          self.package = 1\n",
    "          reward = 40\n",
    "          #print(\"picked up\", self.timestep)\n",
    "          #self.pickup += 1\n",
    "        elif self.package: #Picked up while holding a package\n",
    "          reward = -100\n",
    "        else: # Picked up in wrong location\n",
    "          reward = -10\n",
    "\n",
    "      if action == 5:\n",
    "        #Drop off\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and self.package == 1: #Dropped off in right location\n",
    "          reward = 100\n",
    "          #print(\"dropped off\", self.timestep)\n",
    "          self.package = 0\n",
    "          self.timestep = self.max_timesteps\n",
    "        elif self.package == 0: #Dropped off without holding a package\n",
    "          reward = -100\n",
    "        else: #dropped off in wrong location\n",
    "          reward -10\n",
    "\n",
    "      if np.array_equal(self.agent_pos, self.wall_1) or np.array_equal(self.agent_pos, self.wall_2) or np.array_equal(self.agent_pos, self.wall_3) or np.array_equal(self.agent_pos, self.wall_4) or np.array_equal(self.agent_pos, self.wall_5):\n",
    "        reward = -20\n",
    "        self.agent_pos = prev\n",
    "        #print(\"wall\")\n",
    "\n",
    "      # Comment this to demonstrate the truncation condition.\n",
    "      if self.agent_pos[0] > 5 or self.agent_pos[0] < 0 or self.agent_pos[1] > 5 or self.agent_pos[1] < 0:\n",
    "        reward = -25\n",
    "        #print(\"bounding: \", self.agent_pos)\n",
    "      self.agent_pos = np.clip(self.agent_pos, 0, 5)\n",
    "\n",
    "      self.state = np.zeros((6,6))\n",
    "      self.state[tuple(self.goal_pos)] = 23\n",
    "      self.state[tuple(self.package_pos)] = 5\n",
    "      self.state[tuple(self.wall_1)] = 12\n",
    "      self.state[tuple(self.wall_2)] = 12\n",
    "      self.state[tuple(self.wall_3)] = 12\n",
    "      self.state[tuple(self.wall_4)] = 12\n",
    "      self.state[tuple(self.wall_5)] = 12\n",
    "      self.state[tuple(self.agent_pos)] = 50\n",
    "      observation = self.state.flatten()\n",
    "\n",
    "\n",
    "      self.timestep += 1\n",
    "\n",
    "      # Condition to check for termination (episode is over)\n",
    "      terminated = True if self.timestep >= self.max_timesteps else False\n",
    "\n",
    "      # Condition to check if agent is traversing to a cell beyond the permitted cells\n",
    "      # This helps the agent to learn how to behave in a safe and predictable manner\n",
    "      truncated = True if np.all((np.asarray(self.agent_pos) >=0 ) & (np.asarray(self.agent_pos) <= 6)) else False\n",
    "      #print(self.agent_pos)\n",
    "\n",
    "      return observation, reward, terminated, self.package\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "      plt.title('Grid Environment')\n",
    "      plt.imshow(self.state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:16:03.772939Z",
     "start_time": "2024-10-31T21:16:03.760741Z"
    }
   },
   "id": "e8b78d5732a0ed59",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 144)\n",
    "        self.layer2 = nn.Linear(144, 144)\n",
    "        self.layer3 = nn.Linear(144, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:16:32.921214Z",
     "start_time": "2024-10-31T21:16:32.917450Z"
    }
   },
   "id": "3c7e50d22ef85356",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:16:34.566417Z",
     "start_time": "2024-10-31T21:16:34.562447Z"
    }
   },
   "id": "b3451abf3ca2b0ba",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Double_DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, C):\n",
    "    # initialize environment\n",
    "    self.env = GridEnvironment()\n",
    "    \n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = ReplayMemory(N)\n",
    "    \n",
    "    # initialize networks\n",
    "    self.policy_net = Net(2, 6).to(device)\n",
    "    self.target_net = Net(2, 6).to(device)\n",
    "    \n",
    "    # initialize the optimizer \n",
    "    self.optimizer = optim.SGD(self.policy_net.parameters(), lr=0.01)\n",
    "\n",
    "    self.C = C\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    TAU = .0004\n",
    "    for i in range(episodes):\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      seq  = [None , None]\n",
    "      seq[0] = torch.tensor([0, 0],device=device, dtype=torch.float32).unsqueeze(0)\n",
    "      terminated = False\n",
    "      pos = package = rewards = 0\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      self.env.reset()\n",
    "      steps = 0\n",
    "      while not terminated:\n",
    "        # Select action\n",
    "        action_type = action_function(eps, pos, package)\n",
    "        state = torch.tensor([pos, package], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        # Set sequence\n",
    "        seq[1] = state\n",
    "        # Execute action and observe reward\n",
    "        position, reward, terminated, package = self.env.step(action_type.item())\n",
    "        pos = np.where(position == 50)[0][0]\n",
    "        rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        # store transition in replay buffer\n",
    "        self.replay.push(seq[0], action_type,  seq[1], reward)\n",
    "        seq[0] = state\n",
    "        steps += 1 \n",
    "        \n",
    "        for update_steps in range(steps):\n",
    "            self.optimize_function(discount)\n",
    "            \n",
    "            if steps % self.C == 0:\n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]* TAU + target_net_state_dict[key]*(1- TAU)\n",
    "                \n",
    "        \n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "\n",
    "      # Decay epsilon after every episode\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "  def warehouse_action(self, epsilon, pos, package):\n",
    "      if np.random.rand() < epsilon:\n",
    "        action_type = np.random.randint(6)\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor([pos, package], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "            return self.policy_net(state).max(1).indices.view(1, 1) \n",
    "      return torch.tensor([[action_type]], device=device, dtype=torch.long)\n",
    "      \n",
    "  def optimize_function(self, discount):\n",
    "    BATCH_SIZE = 128\n",
    "    if len(self.replay) < BATCH_SIZE:\n",
    "        return\n",
    "    else:\n",
    "        transitions = self.replay.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        expected_state_action_values = (next_state_values * discount) + reward_batch\n",
    "             \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "              \n",
    "\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(\"pickles/\" + filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net, file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:32:23.742506Z",
     "start_time": "2024-10-31T21:32:23.731396Z"
    }
   },
   "id": "b0de37d720a6b604",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Prints the reward per episode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - int(min(reward_per_episode)) * (.15)\n",
    "    maxs = int(max(reward_per_episode)) + int(max(reward_per_episode)) * (.3) \n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:26:59.006790Z",
     "start_time": "2024-10-31T21:26:59.001177Z"
    }
   },
   "id": "f91389649d5e4348",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m discount \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.4\u001B[39m\n\u001B[0;32m      9\u001B[0m action \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mwarehouse_action\n\u001B[1;32m---> 10\u001B[0m total_rewards \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m agent\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrpreisl_angustsa_assignment2_part3_double_dqn_gridworld.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     12\u001B[0m reward_print(total_rewards, episodes, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrid world\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[11], line 47\u001B[0m, in \u001B[0;36mDouble_DQN.train\u001B[1;34m(self, episodes, epsilon, discount, action_function, greedy)\u001B[0m\n\u001B[0;32m     44\u001B[0m steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m update_steps \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(steps):\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiscount\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m steps \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     50\u001B[0m         target_net_state_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_net\u001B[38;5;241m.\u001B[39mstate_dict()\n",
      "Cell \u001B[1;32mIn[11], line 79\u001B[0m, in \u001B[0;36mDouble_DQN.optimize_function\u001B[1;34m(self, discount)\u001B[0m\n\u001B[0;32m     77\u001B[0m transitions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay\u001B[38;5;241m.\u001B[39msample(BATCH_SIZE)\n\u001B[0;32m     78\u001B[0m batch \u001B[38;5;241m=\u001B[39m Transition(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mtransitions))\n\u001B[1;32m---> 79\u001B[0m non_final_mask \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext_state\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbool\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m non_final_next_states \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([s \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mnext_state \u001B[38;5;28;01mif\u001B[39;00m s \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m])\n\u001B[0;32m     81\u001B[0m state_batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(batch\u001B[38;5;241m.\u001B[39mstate)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "C = 10\n",
    "env = GridEnvironment()\n",
    "agent = Double_DQN(N, C)\n",
    "\n",
    "episodes = 100\n",
    "epsilon = .99\n",
    "discount = .4\n",
    "action = agent.warehouse_action\n",
    "total_rewards = agent.train(episodes, epsilon, discount, action, False)\n",
    "agent.save(\"drpreisl_angustsa_assignment2_part3_double_dqn_gridworld.pickle\")\n",
    "reward_print(total_rewards, episodes, \"grid world\")\n",
    "ep_decay(epsilon, episodes)\n",
    "print(\"Best return: \", max(total_rewards))\n",
    "#total_rewards = agent.train(6, epsilon, discount, action, True)\n",
    "#reward_print(total_rewards, 5, \"grid world\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:34:59.223147Z",
     "start_time": "2024-10-31T21:32:25.424394Z"
    }
   },
   "id": "44ed519e9a676361",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
