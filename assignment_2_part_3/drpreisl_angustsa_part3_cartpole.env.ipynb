{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:51.775833Z",
     "start_time": "2024-10-31T19:01:51.771501Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:55.134415Z",
     "start_time": "2024-10-31T19:01:55.130999Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:57.780803Z",
     "start_time": "2024-10-31T19:01:57.776274Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:14:53.327192Z",
     "start_time": "2024-10-31T19:14:53.315948Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, env):\n",
    "    # initialize environment\n",
    "    self.env = env\n",
    "    state, info = self.env.reset()\n",
    "    # initialize replay memory to capacity N\n",
    "    self.pointer = 0\n",
    "    self.policy_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    self.target_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    self.optimizer = optim.AdamW(self.policy_net.parameters(),amsgrad=True ) #auto learning rate\n",
    "    # big replay memory:\n",
    "    self.size = N\n",
    "    self.state_mem = torch.zeros((N, env.observation_space.shape[0]), dtype=torch.float32, device=device)\n",
    "    self.next_state_mem = torch.zeros((self.size, env.observation_space.shape[0]), dtype=torch.float32, device=device)\n",
    "    self.action_mem = torch.zeros(self.size, dtype=torch.int64, device=device)\n",
    "    self.reward_mem = torch.zeros(self.size, dtype=torch.float32, device=device)\n",
    "    self.done_mem = torch.zeros(self.size, dtype=torch.float32, device=device)\n",
    "    self.pointer = 0\n",
    "\n",
    "  def append(self, state, action, reward, next_state, done):\n",
    "    i = self.pointer % self.size #get index\n",
    "    self.state_mem[i] = state\n",
    "    self.next_state_mem[i] = next_state\n",
    "    self.reward_mem[i] = reward \n",
    "    self.done_mem[i] = 1-done\n",
    "    self.action_mem[i] = action\n",
    "    self.pointer +=1\n",
    "    \n",
    "  def sample(self,batch):\n",
    "    mem = min(self.pointer,self.size) # get range to choose mem from\n",
    "    batch = np.random.choice(mem,batch) # choose random indices\n",
    "    states = self.state_mem[batch]\n",
    "    next_states = self.next_state_mem[batch]\n",
    "    actions = self.action_mem[batch]\n",
    "    rewards = self.reward_mem[batch]\n",
    "    done = self.done_mem[batch]\n",
    "    return states, actions, rewards, next_states, done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    TAU = .004\n",
    "    for i in range(episodes):\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      seq  = [None , None]\n",
    "      state, info = self.env.reset()\n",
    "      seq[0] = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "      done = False\n",
    "      rewards = 0\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      while not done:\n",
    "        # Select action\n",
    "        action_type = action_function(seq[0], eps)\n",
    "        observation, reward, terminated, truncated, _ = self.env.step(action_type.item())\n",
    "        state = torch.tensor(observation, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        # Set sequence\n",
    "        seq[1] = state\n",
    "        self.append(state=seq[0], action=action_type, reward=reward, next_state=seq[1], done=terminated)\n",
    "        if terminated:\n",
    "           seq[1] = None\n",
    "        rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        # store transition in replay buffer\n",
    "        seq[0] = state\n",
    "        \n",
    "        self.r(discount)\n",
    "\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]* TAU + target_net_state_dict[key]*(1- TAU)\n",
    "        done = truncated or terminated\n",
    "      # Decay epsilon after every episode\n",
    "      # epsilon *= epsilon\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "      \n",
    "  def action(self, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_type = self.env.action_space.sample()\n",
    "    else:\n",
    "        # select max(Q)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).max(1).indices.view(1, 1) \n",
    "    return torch.tensor([[action_type]], device=device, dtype=torch.long)\n",
    "        \n",
    "  def r(self,discount):\n",
    "      BATCH_SIZE = 256\n",
    "      if self.pointer < BATCH_SIZE:\n",
    "          return\n",
    "      # Sample a batch from replay memory\n",
    "      states, actions, rewards, next_states, dones = self.sample(BATCH_SIZE)\n",
    "      \n",
    "      # Convert to tensors\n",
    "      state_batch = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "      action_batch = torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(1)\n",
    "      reward_batch = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "      next_state_batch = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "      done_mask = torch.tensor(dones, dtype=torch.float32, device=device) # zeros out terminated\n",
    "      \n",
    "      # Calculate state-action values\n",
    "      state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "      \n",
    "      # Calculate next state values\n",
    "      next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "      with torch.no_grad():\n",
    "          next_state_values[done_mask.bool()] = self.target_net(next_state_batch[done_mask.bool()]).max(1).values\n",
    "      \n",
    "      # Expected Q values\n",
    "      expected_state_action_values = (next_state_values * discount) + reward_batch\n",
    "      \n",
    "      # Compute loss\n",
    "      criterion = nn.SmoothL1Loss()\n",
    "      loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "      \n",
    "      # Optimize the model\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "  \n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net.state_dict(), file,protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:19:10.107111Z",
     "start_time": "2024-10-31T19:19:10.101012Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prints the reward per epsisode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - int(min(reward_per_episode)) * (.15)\n",
    "    maxs = int(max(reward_per_episode)) + int(max(reward_per_episode)) * (.3) \n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:29:55.191731Z",
     "start_time": "2024-10-31T19:29:40.566273Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N = 5000\n",
    "# env = gym.make(\"MountainCar-v0\")\n",
    "# env.reset()\n",
    "# MtnCar = DQN(N, env)\n",
    "\n",
    "# episodes = 20\n",
    "# epsilon = .8\n",
    "# discount = .4\n",
    "# action = MtnCar.mountain_car_action\n",
    "# total_rewards = MtnCar.train(episodes, epsilon, discount, action, False)\n",
    "# # MtnCar.save(\"drpreisl_angustsa_assignment2_part2_dqn_mountaincar.pickle\")\n",
    "# reward_print(total_rewards, episodes, \"MountainCar\")\n",
    "# ep_decay(epsilon, episodes)\n",
    "# total_rewards = MtnCar.train(6, epsilon, discount, action, True)\n",
    "# reward_print(total_rewards, 5, \"MountainCar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T21:41:36.703999Z",
     "start_time": "2024-10-31T21:41:36.241784Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_34344\\2266118280.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_batch = torch.tensor(states, dtype=torch.float32, device=device)\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_34344\\2266118280.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_batch = torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(1)\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_34344\\2266118280.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reward_batch = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_34344\\2266118280.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state_batch = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_34344\\2266118280.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  done_mask = torch.tensor(dones, dtype=torch.float32, device=device) # zeros out terminated\n"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "cart = DQN(N, env)\n",
    "\n",
    "episodes = 2000\n",
    "epsilon = .995\n",
    "discount = .99\n",
    "action = cart.action\n",
    "total_rewards = cart.train(episodes, epsilon, discount, action, False)\n",
    "# cart.save(\"drpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\")\n",
    "reward_print(total_rewards, episodes, \"CartPole\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = cart.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'layer1.weight': tensor([[-3.7587e-04,  2.8727e-02,  1.5497e+00,  5.6704e-02],\n",
      "        [ 2.0399e-01,  6.0099e-02,  2.4200e-01,  3.0197e-01],\n",
      "        [ 1.5205e-01, -2.2144e-01,  2.2754e-01,  1.7968e-01],\n",
      "        [ 2.3476e-01,  3.5045e-01,  1.9970e-01, -9.1124e-02],\n",
      "        [-6.8041e-02,  1.9302e-01,  1.7375e-01, -3.2185e-01],\n",
      "        [ 1.8317e-01,  4.1957e-01, -2.1138e-01,  1.1632e-01],\n",
      "        [ 8.5409e-02,  7.4719e-02, -2.2761e-02,  1.7634e-02],\n",
      "        [ 1.9209e-01, -2.7605e-01,  3.6012e-02,  3.5498e-01],\n",
      "        [ 2.6168e-01,  8.6391e-02, -1.5612e-01, -2.6951e-04],\n",
      "        [-1.7417e-01, -1.1104e-01,  4.7871e-02, -3.2893e-01],\n",
      "        [-9.5106e-02, -2.0484e-01, -2.1683e-01, -8.6860e-02],\n",
      "        [-2.3065e-01,  2.2802e-01,  2.0182e-01, -2.2248e-02],\n",
      "        [ 2.5183e-01,  1.8448e-01, -3.4197e-02,  9.5328e-02],\n",
      "        [-2.9238e-01, -2.5563e-01,  1.2405e-01,  3.0904e-01],\n",
      "        [-1.1714e-01,  3.7279e-01, -4.7538e-01,  6.6922e-02],\n",
      "        [-1.8742e-02,  2.5841e-01, -2.5243e-02,  5.0168e-02],\n",
      "        [-7.7137e-02,  1.3614e-01, -4.0234e-01, -3.2158e-01],\n",
      "        [-1.7033e-01, -2.0271e-01,  4.7023e-02, -5.7546e-02],\n",
      "        [-1.5221e-01,  2.8611e-01,  8.6675e-02, -1.4009e-02],\n",
      "        [ 1.7299e-01,  1.4487e-02,  1.7698e-01, -1.4947e-02],\n",
      "        [ 1.4198e-01, -2.4073e-01, -3.7056e-02,  2.8489e-01],\n",
      "        [-2.3154e-01,  5.8462e-03, -2.2277e-01,  4.3471e-02],\n",
      "        [ 3.6747e-01, -2.0714e-01,  1.3302e-01,  1.7125e-01],\n",
      "        [ 1.9987e-01,  4.0952e-03, -6.8726e-01, -8.2535e-02],\n",
      "        [-3.1694e-02, -3.5352e-01, -1.7137e-01, -1.4271e-01],\n",
      "        [-7.2598e-02,  2.8741e-01, -8.2838e-02,  2.0160e-01],\n",
      "        [-1.0543e-01,  1.3976e-01,  1.5310e-01, -4.2666e-02],\n",
      "        [-3.3651e-01,  3.2284e-01, -1.4655e-01, -1.4874e-01],\n",
      "        [ 5.5530e-02, -2.7429e-01, -2.4567e-01,  1.4504e-01],\n",
      "        [-2.3857e-01, -6.9308e-02, -8.0882e-02, -2.4483e-01],\n",
      "        [-7.1399e-02,  1.8692e-02, -2.0971e-01, -3.1481e-01],\n",
      "        [-2.3421e-01, -1.8181e-01,  1.0495e-02, -3.6999e-03]], device='cuda:0'), 'layer1.bias': tensor([-0.3263,  0.0732, -0.1833, -0.1329,  0.1239, -0.0380, -0.1722,  0.1832,\n",
      "         0.1743, -0.1507,  0.3568,  0.3992,  0.4073, -0.2613,  0.1171,  0.3645,\n",
      "        -0.0070,  0.3802, -0.2471, -0.1775,  0.0097, -0.1329,  0.3735, -0.2301,\n",
      "        -0.3014, -0.2096,  0.2913, -0.0631,  0.0524,  0.4147, -0.1433, -0.3546],\n",
      "       device='cuda:0'), 'layer2.weight': tensor([[ 1.7628e-01,  4.2045e-02,  9.7307e-02,  ..., -1.2248e-01,\n",
      "         -9.1546e-02, -6.6321e-03],\n",
      "        [-2.6052e+00,  3.7040e-03, -1.5284e-01,  ...,  9.1299e-02,\n",
      "          2.3352e-02, -8.7089e-02],\n",
      "        [-2.0191e-01,  5.6006e-02,  2.4166e-02,  ...,  9.9467e-02,\n",
      "          1.9111e-02,  6.7401e-02],\n",
      "        ...,\n",
      "        [-3.4129e-01,  3.3581e-02, -1.0719e-02,  ...,  1.4559e-01,\n",
      "          5.4262e-02, -5.7465e-02],\n",
      "        [-2.8776e-01,  1.0062e-01,  2.9039e-02,  ...,  5.4903e-02,\n",
      "         -1.0102e-01, -6.0563e-02],\n",
      "        [ 6.3190e-02,  1.4066e-03,  1.0398e-01,  ..., -3.0623e-02,\n",
      "          9.9490e-02, -4.6507e-02]], device='cuda:0'), 'layer2.bias': tensor([ 0.0767,  0.0302,  0.1113,  0.1464,  0.1372, -0.0391,  0.0747,  0.1352,\n",
      "        -0.0146,  0.0009, -0.0481,  0.1521,  0.3762, -0.0769,  0.1469, -0.0676,\n",
      "         0.0144,  0.0367,  0.0927, -0.0623, -0.0149,  0.0144, -0.1314,  0.0275,\n",
      "         0.1158,  0.0848,  0.1190,  0.1180,  0.0465,  0.0193,  0.0511,  0.0128],\n",
      "       device='cuda:0'), 'layer3.weight': tensor([[-0.0330,  0.1513,  0.1398,  0.1664,  0.1758, -0.0412, -0.0922,  0.0987,\n",
      "          0.1380, -0.0408, -0.0767,  0.0887,  0.4802, -0.0176,  0.1499,  0.0143,\n",
      "          0.1574, -0.0220, -0.0135, -0.0026, -0.0209, -0.0140, -0.0080,  0.1650,\n",
      "          0.0451,  0.1058,  0.0339, -0.0661,  0.1739,  0.1764,  0.0694, -0.0101],\n",
      "        [-0.0688, -0.0555,  0.1126, -0.0523, -0.0298, -0.0536,  0.0391, -0.0174,\n",
      "         -0.0790,  0.0807,  0.0788,  0.1182,  0.5680, -0.0145,  0.1074, -0.0414,\n",
      "          0.1208, -0.0107,  0.1446, -0.0626, -0.0532, -0.0150, -0.0062,  0.0096,\n",
      "          0.1931,  0.1697,  0.1765,  0.0863,  0.0705,  0.0402,  0.1741, -0.0364]],\n",
      "       device='cuda:0'), 'layer3.bias': tensor([-0.0093, -0.0184], device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "print(cart.policy_net.state_dict())\n",
    "# cart.save(\"drpreisl_angustsa_assignment2_part3_ddqn_cartpole.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
