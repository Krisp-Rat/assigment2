{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:51.775833Z",
     "start_time": "2024-10-31T19:01:51.771501Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Install required libraries\n",
    "# Import required libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:55.134415Z",
     "start_time": "2024-10-31T19:01:55.130999Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs, action):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(obs, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:01:57.780803Z",
     "start_time": "2024-10-31T19:01:57.776274Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:14:53.327192Z",
     "start_time": "2024-10-31T19:14:53.315948Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  # initialize values\n",
    "  def __init__(self, N, env):\n",
    "    # initialize environment\n",
    "    self.env = env\n",
    "    state, info = self.env.reset()\n",
    "    # initialize replay memory to capacity N\n",
    "    self.replay = ReplayMemory(N)\n",
    "    self.pointer = 0\n",
    "    self.policy_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    self.target_net = Net(len(state), self.env.action_space.n).to(device)\n",
    "    self.optimizer = optim.AdamW(self.policy_net.parameters(),amsgrad=True ) #auto learning rate\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  # Main training function\n",
    "  def train(self, episodes, epsilon, discount, action_function, greedy):\n",
    "    total_reward = [0] * episodes  \n",
    "    TAU = .0004\n",
    "    for i in range(episodes):\n",
    "      # if i % 5 == 0:\n",
    "      #    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "      # initialize sequence S and preprocessed sequence o\n",
    "      seq  = [None , None]\n",
    "      state, info = self.env.reset()\n",
    "      seq[0] = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "      done = False\n",
    "      rewards = 0\n",
    "      eps = epsilon ** i if not greedy else 0\n",
    "      step =  0\n",
    "      while not done:\n",
    "        step +=1\n",
    "        # Select action\n",
    "        action_type = action_function(seq[0], eps)\n",
    "        observation, reward, terminated, truncated, _ = self.env.step(action_type.item())\n",
    "        state = torch.tensor(observation, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        # Set sequence\n",
    "        seq[1] = state\n",
    "        if terminated:\n",
    "           seq[1] = None\n",
    "        rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        # store transition in replay buffer\n",
    "        self.replay.push(seq[0], action_type,  seq[1], reward)\n",
    "        seq[0] = state\n",
    "        \n",
    "        self.optimize_function(discount)\n",
    "\n",
    "        if step%10 == 0:\n",
    "          target_net_state_dict = self.target_net.state_dict()\n",
    "          policy_net_state_dict = self.policy_net.state_dict()\n",
    "          for key in policy_net_state_dict:\n",
    "              target_net_state_dict[key] = policy_net_state_dict[key]* TAU + target_net_state_dict[key]*(1- TAU)        \n",
    "          self.target_net.load_state_dict(target_net_state_dict)\n",
    "        done = truncated or terminated\n",
    "      # Decay epsilon after every episode\n",
    "      epsilon *= epsilon\n",
    "      total_reward[i] = rewards  \n",
    "    return total_reward\n",
    "  # Determine the action for the warehouse environment\n",
    "      \n",
    "  def mountain_car_action(self, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_type = self.env.action_space.sample()\n",
    "    else:\n",
    "        # select max(Q)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).max(1).indices.view(1, 1) \n",
    "    return torch.tensor([[action_type]], device=device, dtype=torch.long)\n",
    "  \n",
    "  def optimize_function(self, discount):\n",
    "    BATCH_SIZE = 256\n",
    "    if len(self.replay) < BATCH_SIZE:\n",
    "        return\n",
    "    else:\n",
    "        transitions = self.replay.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        expected_state_action_values = (next_state_values * discount) + reward_batch\n",
    "             \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "  # Save the current weights\n",
    "  def save(self, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "      pickle.dump(self.policy_net.state_dict(), file,protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:19:10.107111Z",
     "start_time": "2024-10-31T19:19:10.101012Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prints the reward per epsisode graph\n",
    "def reward_print(reward_per_episode, episodes, info): \n",
    "    mins = int(min(reward_per_episode)) - int(min(reward_per_episode)) * (.15)\n",
    "    maxs = int(max(reward_per_episode)) + int(max(reward_per_episode)) * (.3) \n",
    "    plt.figure()\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Cumulative Reward', fontsize=20)\n",
    "    plt.title(f'Cumulative Reward Per Episode ({info})', fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin= mins, ymax=maxs)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#prints the epsilon decay graph\n",
    "def ep_decay(eps, episodes):\n",
    "    epsilon_values = [(eps ** i) * 1 for i in range(episodes)]\n",
    "    plt.figure()\n",
    "    plt.plot(epsilon_values, linewidth=4)\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Epsilon Value', fontsize=20)\n",
    "    plt.title(f\"Epsilon Decay for {eps}\", fontsize=24)\n",
    "    plt.xticks([0, episodes * .2, episodes * .4, episodes * .6, episodes * .8, episodes], fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=episodes)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:29:55.191731Z",
     "start_time": "2024-10-31T19:29:40.566273Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N = 5000\n",
    "# env = gym.make(\"MountainCar-v0\")\n",
    "# env.reset()\n",
    "# MtnCar = DQN(N, env)\n",
    "\n",
    "# episodes = 20\n",
    "# epsilon = .8\n",
    "# discount = .4\n",
    "# action = MtnCar.mountain_car_action\n",
    "# total_rewards = MtnCar.train(episodes, epsilon, discount, action, False)\n",
    "# # MtnCar.save(\"drpreisl_angustsa_assignment2_part2_dqn_mountaincar.pickle\")\n",
    "# reward_print(total_rewards, episodes, \"MountainCar\")\n",
    "# ep_decay(epsilon, episodes)\n",
    "# total_rewards = MtnCar.train(6, epsilon, discount, action, True)\n",
    "# reward_print(total_rewards, 5, \"MountainCar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T21:41:36.703999Z",
     "start_time": "2024-10-31T21:41:36.241784Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TAU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m discount \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.99\u001b[39m\n\u001b[0;32m      9\u001b[0m action \u001b[38;5;241m=\u001b[39m cart\u001b[38;5;241m.\u001b[39mmountain_car_action\n\u001b[1;32m---> 10\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mcart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# cart.save(\"drpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\")\u001b[39;00m\n\u001b[0;32m     12\u001b[0m reward_print(total_rewards, episodes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 54\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self, episodes, epsilon, discount, action_function, greedy)\u001b[0m\n\u001b[0;32m     52\u001b[0m   policy_net_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m policy_net_state_dict:\n\u001b[1;32m---> 54\u001b[0m       target_net_state_dict[key] \u001b[38;5;241m=\u001b[39m policy_net_state_dict[key]\u001b[38;5;241m*\u001b[39m \u001b[43mTAU\u001b[49m \u001b[38;5;241m+\u001b[39m target_net_state_dict[key]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m TAU)        \n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mload_state_dict(target_net_state_dict)\n\u001b[0;32m     56\u001b[0m done \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;129;01mor\u001b[39;00m terminated\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TAU' is not defined"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "cart = DQN(N, env)\n",
    "\n",
    "episodes = 600\n",
    "epsilon = .8\n",
    "discount = .99\n",
    "action = cart.mountain_car_action\n",
    "total_rewards = cart.train(episodes, epsilon, discount, action, False)\n",
    "# cart.save(\"drpreisl_angustsa_assignment2_part2_dqn_cartpole.pickle\")\n",
    "reward_print(total_rewards, episodes, \"CartPole\")\n",
    "ep_decay(epsilon, episodes)\n",
    "total_rewards = cart.train(6, epsilon, discount, action, True)\n",
    "reward_print(total_rewards, 5, \"CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'layer1.weight': tensor([[ 3.5433e-01,  2.7465e-01,  7.5301e-01, -1.6675e-02],\n",
      "        [-7.4364e-01, -1.3244e+00, -2.1997e+00, -9.4949e-01],\n",
      "        [ 1.0396e-01, -1.6031e-01,  2.6495e-01,  5.4392e-02],\n",
      "        [-2.6355e-01, -5.3193e-02, -1.0929e-01,  1.0894e-02],\n",
      "        [ 2.6695e-01,  2.0126e-01, -1.1025e+00, -3.0423e-01],\n",
      "        [ 6.2737e-01, -2.3406e-01, -3.8444e-01,  5.4150e-02],\n",
      "        [-6.7813e-03,  8.7717e-02,  6.1036e-02, -3.8656e-02],\n",
      "        [ 2.0772e-01, -1.8139e-01,  2.4778e-01, -2.3355e-01],\n",
      "        [ 5.8415e-01,  2.6104e-01, -6.1396e-01, -3.4071e-01],\n",
      "        [ 6.0256e-02,  4.1118e-01, -6.1339e-01,  2.0220e-01],\n",
      "        [ 1.2182e-01, -1.8772e-01,  7.9857e-01, -9.8625e-03],\n",
      "        [ 1.6319e-01, -2.2112e-02, -3.9875e-01, -1.0842e-01],\n",
      "        [-1.0934e-01,  2.8547e-02,  3.4172e-01,  1.3674e-01],\n",
      "        [ 1.7865e-01, -7.1368e-02, -4.3655e-02,  1.0616e-01],\n",
      "        [-3.3848e-01, -1.3433e-01, -2.6145e+00, -5.8274e-01],\n",
      "        [ 9.0321e-02, -2.1949e-01, -4.9829e-01, -8.1663e-02],\n",
      "        [-9.6879e-02,  4.0889e-01,  1.1024e-02,  9.3298e-02],\n",
      "        [ 1.7071e-01,  6.5378e-01,  1.1538e+00,  4.8749e-01],\n",
      "        [-2.1821e-01,  1.5627e-01,  2.1374e-02,  2.6266e-01],\n",
      "        [ 9.6038e-02, -1.2396e-01,  1.6196e-01, -1.9074e-01],\n",
      "        [-1.9358e-01,  4.1282e-01, -4.3752e-01, -3.3122e-01],\n",
      "        [-3.4803e-01,  5.2054e-02,  6.1384e-02,  2.1588e-01],\n",
      "        [-1.1452e+00, -3.6901e-01,  7.8841e-01,  1.6333e-01],\n",
      "        [-3.0702e-01,  1.8464e-01,  6.7258e-01,  2.9782e-01],\n",
      "        [ 1.4119e-01, -1.7319e-01,  6.8118e-02, -2.2279e-01],\n",
      "        [ 5.2173e-03,  3.6484e-01,  4.9438e-01,  3.6979e-01],\n",
      "        [-6.6667e-02,  5.5273e-02,  1.1155e-01,  2.9561e-03],\n",
      "        [-5.2389e-02, -1.0607e-01, -3.0661e+00, -2.8087e-01],\n",
      "        [ 4.8401e-02,  6.9259e-02,  1.4534e-02,  1.1073e-01],\n",
      "        [ 5.3905e-02,  3.3653e-01, -1.7999e-01, -1.8388e-01],\n",
      "        [-2.7132e-01, -4.6458e-02,  2.1489e-01,  1.1200e-01],\n",
      "        [-2.4650e-01, -4.1788e-02,  8.6766e-01,  4.4312e-01]], device='cuda:0'), 'layer1.bias': tensor([ 0.5260, -0.1658,  0.8480,  0.7961,  0.4021,  0.5238,  0.7065,  0.5651,\n",
      "         0.3135,  0.6085,  0.7001,  0.6311,  0.7575,  0.7418, -0.3117,  0.6938,\n",
      "         0.8993, -0.2955,  0.7540,  0.5595,  0.6107,  0.9304, -0.5214,  0.9581,\n",
      "         0.6246,  0.8765, -0.1740, -0.5620, -0.3333,  0.5765,  0.8397, -0.3463],\n",
      "       device='cuda:0'), 'layer2.weight': tensor([[ 0.5167, -0.8341,  0.1377,  ...,  0.0573,  0.1114,  0.7051],\n",
      "        [ 0.9070,  0.2232, -0.1156,  ...,  0.0738,  0.0514,  0.4352],\n",
      "        [ 0.4657, -1.4556,  0.4679,  ...,  0.3456,  0.6409, -0.3420],\n",
      "        ...,\n",
      "        [-0.1045, -0.0189, -0.0442,  ...,  0.0434, -0.0249,  0.1127],\n",
      "        [ 0.5300, -1.7900,  0.4735,  ...,  0.3384,  0.6273, -0.5521],\n",
      "        [ 0.4498, -1.5189,  0.5337,  ...,  0.2210,  0.5004, -0.3151]],\n",
      "       device='cuda:0'), 'layer2.bias': tensor([ 1.5079e-01,  1.9672e-02,  4.9734e-01, -1.0480e-01, -6.3687e-02,\n",
      "         5.4724e-01,  8.6520e-02,  2.0020e-02,  4.0752e-01,  4.1289e-01,\n",
      "         5.1695e-01, -1.0090e-01,  2.1696e-02,  4.5500e-01,  5.3671e-02,\n",
      "         3.8548e-01,  3.7848e-01,  6.1424e-01,  7.6912e-05, -1.9596e-01,\n",
      "        -3.1493e-01,  3.8877e-01,  5.7803e-01,  4.6727e-01,  3.2770e-02,\n",
      "         3.7059e-01,  2.4208e-02, -1.0061e-01, -1.4414e-01, -6.1873e-02,\n",
      "         5.3326e-01,  5.4762e-01], device='cuda:0'), 'layer3.weight': tensor([[-1.3067, -0.5480,  0.6056, -0.3049,  0.0121,  0.5751,  0.0903, -0.0911,\n",
      "          0.5764,  0.7106,  0.6506,  0.0761, -0.0209,  0.5215, -0.0658,  0.6697,\n",
      "          0.6746,  0.5830,  0.0708, -0.1418, -0.9054,  0.6536,  0.5644,  0.6405,\n",
      "          0.0763,  0.5905, -0.3971, -1.0424,  0.1171, -0.0038,  0.5119,  0.6039],\n",
      "        [-0.8752,  0.3423,  0.5001, -0.8172,  0.0342,  0.5187, -0.0520, -0.1063,\n",
      "          0.6398,  0.5834,  0.6055,  0.0244, -0.1079,  0.7191,  0.1187,  0.6512,\n",
      "          0.6662,  0.5447, -0.0474, -1.8364, -1.6547,  0.6589,  0.5199,  0.4098,\n",
      "         -0.0139,  0.7902, -0.6847, -1.1650, -0.0660, -0.0263,  0.6712,  0.5905]],\n",
      "       device='cuda:0'), 'layer3.bias': tensor([0.3620, 0.2147], device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "print(cart.policy_net.state_dict())\n",
    "cart.save(\"drpreisl_angustsa_assignment2_part3_ddqn_cartpole.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
